{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project to automate api access \n",
    "+ I added my api key as config.py file  PLease get your api key https://newsapi.org/docs/get-started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import config\n",
    "import math\n",
    "from newsapi import NewsApiClient\n",
    "import newspaper\n",
    "import requests\n",
    "from newspaper import fulltext\n",
    "import time\n",
    "# Hit Api with credentials\n",
    "newsapi = NewsApiClient(api_key=config.api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all sources\n",
    "+ read through available sources list and make df storing domain and source name\n",
    "+ I hit a ton of sources below, we can clearly narrow it down "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing data sources\n",
    "+ Lets attempt to grab some sources from different geographic locations as well as different idological perspectives\n",
    "\n",
    "+ categorizing news sources\n",
    "    + Traditional TV MSM\n",
    "        +  http://us.cnn.com   \n",
    "        +  http://www.cnbc.com \n",
    "        +  http://www.foxnews.com  \n",
    "        +  http://www.msnbc.com  \n",
    "        +  https://abcnews.go.com  \n",
    "        +  http://www.nbcnews.com  \n",
    "    + Traditional publications \n",
    "        +  http://www.nytimes.com  \n",
    "        +  https://www.washingtonpost.com \n",
    "        \n",
    "    + Internet Sources\n",
    "        +  http://www.huffingtonpost.com \n",
    "        +  https://www.politico.com\n",
    "        +  http://www.breitbart.com \n",
    "        +  https://news.google.com \n",
    "        +  https://www.buzzfeed.com \n",
    "        +  https://news.vice.com  \n",
    "    + Financial publications\n",
    "        +  http://www.economist.com\n",
    "        +  http://www.bloomberg.com \n",
    "        +  http://www.businessinsider.com \n",
    "        +  http://www.wsj.com\n",
    "        +  http://fortune.com  \n",
    "        \n",
    "    + News aggregators\n",
    "        +  https://apnews.com/ \n",
    "        +  http://www.reuters.com \n",
    "    + foreign reporting\n",
    "         + http://www.aljazeera.com  \n",
    "         + http://www.bbc.co.uk/news   \n",
    "         + https://www.jpost.com/  \n",
    "         + http://timesofindia.indiatimes.com \n",
    "         + https://russian.rt.com \n",
    "         + https://www.theguardian.com/uk \n",
    "         + http://www.independent.co.uk  \n",
    "         + http://www.telegraph.co.uk  \n",
    "\n",
    "\n",
    " \n",
    "### Build Query String "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fox-news,google-news,msnbc'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/mburke65/Data_698_Final/master/lists.csv'\n",
    "df = pd.read_csv(url, error_bad_lines=False)\n",
    "\n",
    "\n",
    "## Literally picking data sources from df i printed above \n",
    "a= df.iloc[[16,17,24],:]\n",
    "b= df.iloc[[40,43,77],:]\n",
    "list_sources =a[\"sources\"].tolist()\n",
    "\n",
    "## build out string for query request \n",
    "myString = \",\".join(list_sources)\n",
    "\n",
    "list_sources2 = b[\"sources\"].tolist()\n",
    "myString2 = \",\".join(list_sources2)\n",
    "myString2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets begin process of automating query calls\n",
    "+ first lets build function to clean query returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(query):\n",
    "    for x in query['articles']:\n",
    "        try:\n",
    "            x[\"source\"] = x[\"source\"][\"name\"]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            x['publishedAt'] = str.split(x['publishedAt'], \"T\")[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del x['urlToImage']\n",
    "        except KeyError:\n",
    "            pass\n",
    "    my_df = pd.DataFrame(query[\"articles\"])\n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to hit the api\n",
    "+ Originally I had a loop here.  instead I figured I would just build a function that takes start data, end data, query term(candidate)\n",
    "    + The original code kept giving me a query limit reached result, so I decided to change up strategy and search one day at a time\n",
    "    + After we hit the papers at the start for past 30 days, we will only need 1 day at a time going forward.\n",
    "    + I built in some print statements for error handeling, which you will see below in the block after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "candidates_list=[]\n",
    "total_count=[]\n",
    "\n",
    "# Make first call\n",
    "def hit_api(start,end,q,myString):\n",
    "    \n",
    "    ## catch bug with formatted strings for dates\n",
    "    if end < 10:\n",
    "        start_str = \"0\"+ str(start)\n",
    "        end_str = \"0\"+ str(end)\n",
    "    elif end==10:\n",
    "        start_str = \"0\"+ str(start)\n",
    "        end_str = str(end)\n",
    "    else :\n",
    "        start_str = str(start)\n",
    "        end_str = str(end)\n",
    "      \n",
    "    ## API query\n",
    "    all_articles = newsapi.get_everything(q=q,\n",
    "                                          sources=myString,\n",
    "                                          language='en',\n",
    "                                          from_param='2019-09-{}'.format(start_str),\n",
    "                                          to='2019-09-{}'.format(end_str),\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=100,\n",
    "                                          page=1)\n",
    "    \n",
    "    ## get count\n",
    "    total_pages = math.ceil(all_articles[\"totalResults\"]/100)\n",
    "    print(\"query will return: \"+ str(all_articles[\"totalResults\"]))\n",
    "    \n",
    "    ## store count to check versus dimension of df later\n",
    "    total_count.append(all_articles[\"totalResults\"])\n",
    "    \n",
    "    ## Clen query \n",
    "    all_articles = clean_query(all_articles)\n",
    "    \n",
    "    ## append to list\n",
    "    candidates_list.append(all_articles)\n",
    "    return(candidates_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built out a loop \n",
    "+ Simple, look at first day of September to last day incrementing start and end by 1 each time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query will return: 0\n",
      "query will return: 2\n",
      "query will return: 4\n",
      "query will return: 3\n",
      "query will return: 3\n",
      "query will return: 3\n",
      "query will return: 2\n",
      "query will return: 2\n",
      "query will return: 1\n",
      "query will return: 3\n",
      "query will return: 39\n"
     ]
    }
   ],
   "source": [
    "for x in range(20,31):\n",
    "    df=hit_api(x,x+1,'Biden OR Joe Biden',myString)\n",
    "    \n",
    "## collapse list on itself to build big df\n",
    "Bernie_df = pd.concat(df)\n",
    "Bernie_df= Bernie_df.reset_index(drop=True)\n",
    "\n",
    "for x in range(20, 31):\n",
    "    df = hit_api(x, x+1, 'Biden OR Joe Biden', myString)\n",
    "\n",
    "# collapse list on itself to build big df\n",
    "Cand_df = pd.concat(df)\n",
    "Cand_df = Cand_df.reset_index(drop=True)\n",
    "error_checks()\n",
    "\n",
    "\n",
    "## Build counts as check on progress of loop. will count down Url left to hit\n",
    "counts=Cand_df['url'].shape[0]\n",
    "list_full_text=[]\n",
    "for link in Cand_df['url']:\n",
    "    counts-=1\n",
    "    print(counts)\n",
    "    html = requests.get(link,headers=headers).text\n",
    "    try: text = fulltext(html)\n",
    "    except: \n",
    "        print(\"no words found\")\n",
    "        text=\"no words found\"\n",
    "        list_full_text.append(text)\n",
    "        continue\n",
    "    list_full_text.append(text)\n",
    "Cand_df['full_art']=pd.Series(list_full_text)\n",
    "Cand_df.to_csv('data_csvs/data1'+str(time.time())+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sanity checks\n",
    "\n",
    "### Issues\n",
    "+ The counts don't match up between expected and actual because the query results are limited to first 100 hits\n",
    "    + Won't be an issue with less publications, but we need to be careful splitting candidate calls up.\n",
    "+ Not all links are unique\n",
    "    + This is expected given internet sources reposting articles from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we should expect: 62 articles\n",
      "we have: 62 articles\n",
      "0 results came into query that exceeded 100 hits in a day.\n",
      "we have 49 unique links\n",
      "we have duplciate shape df of: (13, 7) \n"
     ]
    }
   ],
   "source": [
    "def error_checks():\n",
    "    ##validate that df shape[0] is equal to expected query count\n",
    "    print(\"we should expect: {} articles\".format(sum(total_count)))\n",
    "    print(\"we have: {} articles\".format(Bernie_df.shape[0]))\n",
    "\n",
    "\n",
    "    ## test 100 results theory\n",
    "    over_100 = sum([x-100 for x in total_count if x>100 ])\n",
    "    print(\"{} results came into query that exceeded 100 hits in a day.\".format(over_100))\n",
    "\n",
    "\n",
    "    ##unique links\n",
    "    unique_array = Bernie_df.url.unique()\n",
    "    print(\"we have {} unique links\".format(unique_array.shape[0]))\n",
    "    \n",
    "    duplicateRowsDF = Bernie_df[Bernie_df.duplicated(\"url\")]\n",
    "    print(\"we have duplciate shape df of: {} \".format(duplicateRowsDF.shape))\n",
    "\n",
    "\n",
    "error_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hitting newspaper 3k Api with links\n",
    "+ Count print showing amount articles left\n",
    "+ No words found prints it article has no words\n",
    "+ headers needs to be custom changed to your own headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "60\n",
      "59\n",
      "58\n",
      "57\n",
      "56\n",
      "55\n",
      "54\n",
      "53\n",
      "52\n",
      "51\n",
      "50\n",
      "49\n",
      "48\n",
      "47\n",
      "46\n",
      "45\n",
      "44\n",
      "43\n",
      "42\n",
      "41\n",
      "40\n",
      "39\n",
      "38\n",
      "37\n",
      "36\n",
      "35\n",
      "34\n",
      "33\n",
      "32\n",
      "31\n",
      "30\n",
      "29\n",
      "28\n",
      "27\n",
      "26\n",
      "25\n",
      "24\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}\n",
    "## Build counts as check on progress of loop. will count down Url left to hit\n",
    "counts=Bernie_df['url'].shape[0]\n",
    "list_full_text=[]\n",
    "for link in Bernie_df['url']:\n",
    "    counts-=1\n",
    "    print(counts)\n",
    "    html = requests.get(link,headers=headers).text\n",
    "    try: text = fulltext(html)\n",
    "    except: \n",
    "        print(\"no words found\")\n",
    "        text=\"no words found\"\n",
    "        list_full_text.append(text)\n",
    "        continue\n",
    "    list_full_text.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernie_df[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add text to previous dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernie_df['full_art']=pd.Series(list_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did this happen?\\n\\nPlease make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bernie_df['full_art'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "Bernie_df.to_csv('Bloomberg'+str(time.time())+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "+ we need to functionize streamline and clean up query calls.\n",
    "+ sorry my python is rusty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernie_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=pd.read_csv(\"data_1571550604.259001.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.loc[119,'full_art']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
