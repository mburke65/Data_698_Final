{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project to automate api access \n",
    "+ I added my api key as config.py file  PLease get your api key https://newsapi.org/docs/get-started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import config\n",
    "import math\n",
    "import csv\n",
    "import sys\n",
    "from newsapi import NewsApiClient\n",
    "import newspaper\n",
    "import requests\n",
    "from newspaper import fulltext\n",
    "import time\n",
    "# Hit Api with credentials\n",
    "newsapi = NewsApiClient(api_key=config.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all sources\n",
    "+ read through available sources list and make df storing domain and source name\n",
    "+ I hit a ton of sources below, we can clearly narrow it down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_checks():\n",
    "    ##validate that df shape[0] is equal to expected query count\n",
    "    print(\"we should expect: {} articles\".format(sum(total_count)))\n",
    "    print(\"we have: {} articles\".format(Bernie_df.shape[0]))\n",
    "\n",
    "\n",
    "    ## test 100 results theory\n",
    "    over_100 = sum([x-100 for x in total_count if x>100 ])\n",
    "    print(\"{} results came into query that exceeded 100 hits in a day.\".format(over_100))\n",
    "\n",
    "\n",
    "    ##unique links\n",
    "    unique_array = Bernie_df.url.unique()\n",
    "    print(\"we have {} unique links\".format(unique_array.shape[0]))\n",
    "    \n",
    "    duplicateRowsDF = Bernie_df[Bernie_df.duplicated(\"url\")]\n",
    "    print(\"we have duplciate shape df of: {} \".format(duplicateRowsDF.shape))\n",
    "\n",
    "def clean_query(query):\n",
    "    for x in query['articles']:\n",
    "        try:\n",
    "            x[\"source\"] = x[\"source\"][\"name\"]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            x['publishedAt'] = str.split(x['publishedAt'], \"T\")[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del x['urlToImage']\n",
    "        except KeyError:\n",
    "            pass\n",
    "    my_df = pd.DataFrame(query[\"articles\"])\n",
    "    return my_df\n",
    "\n",
    "def hit_api(start, end, q, myString2):\n",
    "\n",
    "    # catch bug with formatted strings for dates\n",
    "    if end < 10:\n",
    "        start_str = \"0\" + str(start)\n",
    "        end_str = \"0\" + str(end)\n",
    "    elif end == 10:\n",
    "        start_str = \"0\" + str(start)\n",
    "        end_str = str(end)\n",
    "    else:\n",
    "        start_str = str(start)\n",
    "        end_str = str(end)\n",
    "\n",
    "    # API query\n",
    "    all_articles = newsapi.get_everything(q=q,\n",
    "                                          sources=myString2,\n",
    "                                          language='en',\n",
    "                                          from_param='2019-11-{}'.format(\n",
    "                                              start_str),\n",
    "                                          to='2019-11-{}'.format(end_str),\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=100,\n",
    "                                          page=1)\n",
    "\n",
    "    # get count\n",
    "    total_pages = math.ceil(all_articles[\"totalResults\"]/100)\n",
    "    print(\"query will return: \" + str(all_articles[\"totalResults\"]))\n",
    "\n",
    "    # store count to check versus dimension of df later\n",
    "    total_count.append(all_articles[\"totalResults\"])\n",
    "\n",
    "    # Clen query\n",
    "    all_articles = clean_query(all_articles)\n",
    "\n",
    "    # append to list\n",
    "    candidates_list.append(all_articles)\n",
    "    return(candidates_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing data sources\n",
    "+ Lets attempt to grab some sources from different geographic locations as well as different idological perspectives\n",
    "\n",
    "+ categorizing news sources\n",
    "    + Traditional TV MSM\n",
    "        +  http://us.cnn.com   \n",
    "        +  http://www.cnbc.com \n",
    "        +  http://www.foxnews.com  \n",
    "        +  http://www.msnbc.com  \n",
    "        +  https://abcnews.go.com  \n",
    "        +  http://www.nbcnews.com  \n",
    "    + Traditional publications \n",
    "        +  http://www.nytimes.com  \n",
    "        +  https://www.washingtonpost.com \n",
    "        \n",
    "    + Internet Sources\n",
    "        +  http://www.huffingtonpost.com \n",
    "        +  https://www.politico.com\n",
    "        +  http://www.breitbart.com \n",
    "        +  https://news.google.com \n",
    "        +  https://www.buzzfeed.com \n",
    "        +  https://news.vice.com  \n",
    "    + Financial publications\n",
    "        +  http://www.economist.com\n",
    "        +  http://www.bloomberg.com \n",
    "        +  http://www.businessinsider.com \n",
    "        +  http://www.wsj.com\n",
    "        +  http://fortune.com  \n",
    "        \n",
    "    + News aggregators\n",
    "        +  https://apnews.com/ \n",
    "        +  http://www.reuters.com \n",
    "    + foreign reporting\n",
    "         + http://www.aljazeera.com  \n",
    "         + http://www.bbc.co.uk/news   \n",
    "         + https://www.jpost.com/  \n",
    "         + http://timesofindia.indiatimes.com \n",
    "         + https://russian.rt.com \n",
    "         + https://www.theguardian.com/uk \n",
    "         + http://www.independent.co.uk  \n",
    "         + http://www.telegraph.co.uk  \n",
    "\n",
    "\n",
    " \n",
    "### Build Query Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the-wall-street-journal,the-new-york-times'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## import data dictionary form github\n",
    "url = 'https://raw.githubusercontent.com/mburke65/Data_698_Final/master/lists.csv'\n",
    "df = pd.read_csv(url, error_bad_lines=False)\n",
    "\n",
    "# picking data sources from file on github\n",
    "wapo__nym = df.iloc[[124,87],:]\n",
    "wsj__NYT = df.iloc[[ 123, 117],:]\n",
    "list_sources = wapo__nym[\"sources\"].tolist()\n",
    "\n",
    "# build out string for query request\n",
    "myString = \",\".join(list_sources)\n",
    "\n",
    "list_sources2 = wsj__NYT[\"sources\"].tolist()\n",
    "myString2 = \",\".join(list_sources2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets begin process of automating query calls\n",
    "+ first lets build function to clean query returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(query):\n",
    "    for x in query['articles']:\n",
    "        try:\n",
    "            x[\"source\"] = x[\"source\"][\"name\"]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            x['publishedAt'] = str.split(x['publishedAt'], \"T\")[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del x['urlToImage']\n",
    "        except KeyError:\n",
    "            pass\n",
    "    my_df = pd.DataFrame(query[\"articles\"])\n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to hit the api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "candidates_list = []\n",
    "total_count = []\n",
    "\n",
    "# Make first call\n",
    "def hit_api(start, end, q, myString2):\n",
    "\n",
    "    # catch bug with formatted strings for dates\n",
    "    if end < 10:\n",
    "        start_str = \"0\" + str(start)\n",
    "        end_str = \"0\" + str(end)\n",
    "    elif end == 10:\n",
    "        start_str = \"0\" + str(start)\n",
    "        end_str = str(end)\n",
    "    else:\n",
    "        start_str = str(start)\n",
    "        end_str = str(end)\n",
    "\n",
    "    # API query\n",
    "    all_articles = newsapi.get_everything(q=q,\n",
    "                                          sources=myString2,\n",
    "                                          language='en',\n",
    "                                          from_param='2019-11-{}'.format(\n",
    "                                              start_str),\n",
    "                                          to='2019-11-{}'.format(end_str),\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=100,\n",
    "                                          page=1)\n",
    "\n",
    "    # get count\n",
    "    total_pages = math.ceil(all_articles[\"totalResults\"]/100)\n",
    "    print(\"query will return: \" + str(all_articles[\"totalResults\"]))\n",
    "\n",
    "    # store count to check versus dimension of df later\n",
    "    total_count.append(all_articles[\"totalResults\"])\n",
    "\n",
    "    # Clen query\n",
    "    all_articles = clean_query(all_articles)\n",
    "\n",
    "    # append to list\n",
    "    candidates_list.append(all_articles)\n",
    "    return(candidates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built out a loop (Important\n",
    "+ Simple, look at first day of September to last day incrementing start and end by 1 each time\n",
    "+ We need to expand on queires here, simply just change bernie or bernie sanders to Warren or Elizabeth warren, Biden OR Joe Biden etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query will return: 12\n"
     ]
    }
   ],
   "source": [
    "for x in range(20, 21):\n",
    "    df = hit_api(x, x+1, 'Bernie OR Bernie Sanders', myString2)\n",
    "\n",
    "# collapse list on itself to build big df\n",
    "Bernie_df = pd.concat(df)\n",
    "Bernie_df = Bernie_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sanity checks\n",
    "\n",
    "### Issues\n",
    "+ The counts don't match up between expected and actual because the query results are limited to first 100 hits\n",
    "    + Won't be an issue with less publications, but we need to be careful splitting candidate calls up.\n",
    "+ Not all links are unique\n",
    "    + This is expected given internet sources reposting articles from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we should expect: 12 articles\n",
      "we have: 12 articles\n",
      "0 results came into query that exceeded 100 hits in a day.\n",
      "we have 12 unique links\n",
      "we have duplciate shape df of: (0, 7) \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "error_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hitting newspaper 3k Api with links\n",
    "+ Count print showing amount articles left\n",
    "+ No words found prints it article has no words\n",
    "+ headers needs to be custom changed to your own headers\n",
    "    + click here for your useragent header  https://www.whatismybrowser.com/detect/what-is-my-user-agent\n",
    "+ I added error check to catch videos which populates the article with a statement that just reads 'no words found'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}\n",
    "## Build counts as check on progress of loop. will count down Url left to hit\n",
    "counts=Bernie_df['url'].shape[0]\n",
    "list_full_text=[]\n",
    "for link in Bernie_df['url']:\n",
    "    counts-=1\n",
    "    print(counts)\n",
    "    html = requests.get(link,headers=headers).text\n",
    "    try: text = fulltext(html)\n",
    "    except: \n",
    "        print(\"no words found\")\n",
    "        text=\"no words found\"\n",
    "        list_full_text.append(text)\n",
    "        continue\n",
    "    list_full_text.append(text)\n",
    "Bernie_df['full_art']=pd.Series(list_full_text)\n",
    "Bernie_df.to_csv('data1'+str(time.time+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernie_df[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernie_df['full_art']=pd.Series(list_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernie_df.to_csv('data1'+str(time.time+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "+ we need to functionize streamline and clean up query calls.\n",
    "+ sorry my python is rusty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
