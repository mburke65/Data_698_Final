{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things you need\n",
    "+ config file with api key\n",
    "+ Your user agent info can get https://www.whatismybrowser.com/detect/what-is-my-user-agent and replace the headers variable with your info\n",
    "    + header variable is line 10 in both scraping block\n",
    "+ Script splits up your sources into 2 queries and saves them as csv files.  I created a csv folder for them\n",
    "\n",
    "## change query\n",
    "Hit other candidates as well and make sure that the range (20,31) is no more than 30 days ago.  If you get a out of range error just change 20 to whatever day of month it is, or day of month +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('F:\\\\data620\\\\Data_698_Final')\n",
    "import config\n",
    "import math\n",
    "import csv\n",
    "import sys\n",
    "from newsapi import NewsApiClient\n",
    "import newspaper\n",
    "import requests\n",
    "from newspaper import fulltext\n",
    "import time\n",
    "# Hit Api with credentials\n",
    "newsapi = NewsApiClient(api_key=config.api_key)\n",
    "\n",
    "\n",
    "## global variables\n",
    "candidates_list = []\n",
    "total_count = []\n",
    "\n",
    "\n",
    "def error_checks():\n",
    "    ##validate that df shape[0] is equal to expected query count\n",
    "    print(\"we should expect: {} articles\".format(sum(total_count)))\n",
    "    print(\"we have: {} articles\".format(Cand_df.shape[0]))\n",
    "\n",
    "\n",
    "    ## test 100 results theory\n",
    "    over_100 = sum([x-100 for x in total_count if x>100 ])\n",
    "    print(\"{} results came into query that exceeded 100 hits in a day.\".format(over_100))\n",
    "\n",
    "\n",
    "    ##unique links\n",
    "    unique_array = Cand_df.url.unique()\n",
    "    print(\"we have {} unique links\".format(unique_array.shape[0]))\n",
    "    \n",
    "    duplicateRowsDF = Cand_df[Cand_df.duplicated(\"url\")]\n",
    "    print(\"we have duplciate shape df of: {} \".format(duplicateRowsDF.shape))\n",
    "\n",
    "def clean_query(query):\n",
    "    for x in query['articles']:\n",
    "        try:\n",
    "            x[\"source\"] = x[\"source\"][\"name\"]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            x['publishedAt'] = str.split(x['publishedAt'], \"T\")[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del x['urlToImage']\n",
    "        except KeyError:\n",
    "            pass\n",
    "    my_df = pd.DataFrame(query[\"articles\"])\n",
    "    return my_df\n",
    "\n",
    "def hit_api(start, end, q, myStr):\n",
    "\n",
    "    # catch bug with formatted strings for dates\n",
    "    if end < 10:\n",
    "        start_str = \"0\" + str(start)\n",
    "        end_str = \"0\" + str(end)\n",
    "    elif end == 10:\n",
    "        start_str = \"0\" + str(start)\n",
    "        end_str = str(end)\n",
    "    else:\n",
    "        start_str = str(start)\n",
    "        end_str = str(end)\n",
    "\n",
    "    # API query\n",
    "    all_articles = newsapi.get_everything(q=q,\n",
    "                                          sources=myStr,\n",
    "                                          language='en',\n",
    "                                          from_param='2019-11-{}'.format(\n",
    "                                              start_str),\n",
    "                                          to='2019-11-{}'.format(end_str),\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=100,\n",
    "                                          page=1)\n",
    "\n",
    "    # get count\n",
    "    total_pages = math.ceil(all_articles[\"totalResults\"]/100)\n",
    "    print(\"query will return: \" + str(all_articles[\"totalResults\"]))\n",
    "\n",
    "    # store count to check versus dimension of df later\n",
    "    total_count.append(all_articles[\"totalResults\"])\n",
    "\n",
    "    # Clen query\n",
    "    all_articles = clean_query(all_articles)\n",
    "\n",
    "    # append to list\n",
    "    candidates_list.append(all_articles)\n",
    "    return(candidates_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## import data dictionary form github\n",
    "url = 'https://raw.githubusercontent.com/mburke65/Data_698_Final/master/scrapers/lists.csv'\n",
    "df = pd.read_csv(url, error_bad_lines=False)\n",
    "\n",
    "# picking data sources from file on github\n",
    "wapo__nym = df.iloc[[124,87],:]\n",
    "wsj__NYT = df.iloc[[ 123, 117],:]\n",
    "list_sources = wapo__nym[\"sources\"].tolist()\n",
    "\n",
    "# build out string for query request\n",
    "myString = \",\".join(list_sources)\n",
    "\n",
    "list_sources2 = wsj__NYT[\"sources\"].tolist()\n",
    "myString2 = \",\".join(list_sources2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Wall street journal and NYT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query will return: 5\n",
      "query will return: 3\n",
      "query will return: 4\n",
      "query will return: 9\n",
      "query will return: 11\n",
      "query will return: 7\n",
      "query will return: 7\n",
      "query will return: 4\n"
     ]
    },
    {
     "ename": "NewsAPIException",
     "evalue": "{'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 500 requests over a 24 hour period (250 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNewsAPIException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-786703337cb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhit_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Sanders OR Bernie Sanders'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyString2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# collapse list on itself to build big df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mCand_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-771b49d2234f>\u001b[0m in \u001b[0;36mhit_api\u001b[1;34m(start, end, q, myStr)\u001b[0m\n\u001b[0;32m     77\u001b[0m                                           \u001b[0msort_by\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relevancy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                                           \u001b[0mpage_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                                           page=1)\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# get count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\newsapi\\newsapi_client.py\u001b[0m in \u001b[0;36mget_everything\u001b[1;34m(self, q, sources, domains, exclude_domains, from_param, to, language, sort_by, page, page_size)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;31m# Check Status of Request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mNewsAPIException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewsAPIException\u001b[0m: {'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 500 requests over a 24 hour period (250 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}"
     ]
    }
   ],
   "source": [
    "for x in range(8, 30):\n",
    "    df = hit_api(x, x+1, 'Sanders OR Bernie Sanders', myString2)\n",
    "\n",
    "# collapse list on itself to build big df\n",
    "Cand_df = pd.concat(df)\n",
    "Cand_df = Cand_df.reset_index(drop=True)\n",
    "error_checks()\n",
    "\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}\n",
    "## Build counts as check on progress of loop. will count down Url left to hit\n",
    "counts=Cand_df['url'].shape[0]\n",
    "list_full_text=[]\n",
    "for link in Cand_df['url']:\n",
    "    counts-=1\n",
    "    print(counts)\n",
    "    html = requests.get(link,headers=headers).text\n",
    "    try: text = fulltext(html)\n",
    "    except: \n",
    "        print(\"no words found\")\n",
    "        text=\"no words found\"\n",
    "        list_full_text.append(text)\n",
    "        continue\n",
    "    list_full_text.append(text)\n",
    "Cand_df['full_art']=pd.Series(list_full_text)\n",
    "Cand_df.to_csv('data_csvs/data1'+str(time.time())+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape WAPO New York magazine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cand_df=None\n",
    "\n",
    "for x in range(8, 30):\n",
    "    df = hit_api(x, x+1, 'Sanders OR Bernie Sanders', myString)\n",
    "\n",
    "# collapse list on itself to build big df\n",
    "Cand_df = pd.concat(df)\n",
    "Cand_df = Cand_df.reset_index(drop=True)\n",
    "error_checks()\n",
    "\n",
    "\n",
    "## Build counts as check on progress of loop. will count down Url left to hit\n",
    "counts=Cand_df['url'].shape[0]\n",
    "list_full_text=[]\n",
    "for link in Cand_df['url']:\n",
    "    counts-=1\n",
    "    print(counts)\n",
    "    html = requests.get(link,headers=headers).text\n",
    "    try: text = fulltext(html)\n",
    "    except: \n",
    "        print(\"no words found\")\n",
    "        text=\"no words found\"\n",
    "        list_full_text.append(text)\n",
    "        continue\n",
    "    list_full_text.append(text)\n",
    "Cand_df['full_art']=pd.Series(list_full_text)\n",
    "Cand_df.to_csv('data_csvs/data1'+str(time.time())+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
