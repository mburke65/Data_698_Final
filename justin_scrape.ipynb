{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project to automate api access \n",
    "+ I added my api key as config.py file  PLease get your api key https://newsapi.org/docs/get-started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import config\n",
    "import math\n",
    "from newsapi import NewsApiClient\n",
    "import newspaper\n",
    "import requests\n",
    "from newspaper import fulltext\n",
    "import time\n",
    "# Hit Api with credentials\n",
    "newsapi = NewsApiClient(api_key=config.api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all sources\n",
    "+ read through available sources list and make df storing domain and source name\n",
    "+ I hit a ton of sources below, we can clearly narrow it down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domains</th>\n",
       "      <th>sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://abcnews.go.com</td>\n",
       "      <td>abc-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.abc.net.au/news</td>\n",
       "      <td>abc-news-au</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.aftenposten.no</td>\n",
       "      <td>aftenposten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.aljazeera.com</td>\n",
       "      <td>al-jazeera-english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.ansa.it</td>\n",
       "      <td>ansa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.argaam.com</td>\n",
       "      <td>argaam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://arstechnica.com</td>\n",
       "      <td>ars-technica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://arynews.tv/ud/</td>\n",
       "      <td>ary-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://apnews.com/</td>\n",
       "      <td>associated-press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://www.afr.com</td>\n",
       "      <td>australian-financial-review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.axios.com</td>\n",
       "      <td>axios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://www.bbc.co.uk/news</td>\n",
       "      <td>bbc-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://www.bbc.co.uk/sport</td>\n",
       "      <td>bbc-sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://www.bild.de</td>\n",
       "      <td>bild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://br.blastingnews.com</td>\n",
       "      <td>blasting-news-br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>http://www.bleacherreport.com</td>\n",
       "      <td>bleacher-report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>http://www.bloomberg.com</td>\n",
       "      <td>bloomberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>http://www.breitbart.com</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>http://www.businessinsider.com</td>\n",
       "      <td>business-insider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>http://uk.businessinsider.com</td>\n",
       "      <td>business-insider-uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.buzzfeed.com</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>http://www.cbc.ca/news</td>\n",
       "      <td>cbc-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>http://www.cbsnews.com</td>\n",
       "      <td>cbs-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>http://www.cnbc.com</td>\n",
       "      <td>cnbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>http://us.cnn.com</td>\n",
       "      <td>cnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>http://cnnespanol.cnn.com/</td>\n",
       "      <td>cnn-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.ccn.com</td>\n",
       "      <td>crypto-coins-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>http://www.dailymail.co.uk/home/index.html</td>\n",
       "      <td>daily-mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>http://www.tagesspiegel.de</td>\n",
       "      <td>der-tagesspiegel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>http://www.zeit.de/index</td>\n",
       "      <td>die-zeit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>http://t3n.de</td>\n",
       "      <td>t3n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>http://talksport.com</td>\n",
       "      <td>talksport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://techcrunch.com</td>\n",
       "      <td>techcrunch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://techcrunch.cn</td>\n",
       "      <td>techcrunch-cn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>http://www.techradar.com</td>\n",
       "      <td>techradar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>http://www.theamericanconservative.com/</td>\n",
       "      <td>the-american-conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://www.theglobeandmail.com</td>\n",
       "      <td>the-globe-and-mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>http://thehill.com</td>\n",
       "      <td>the-hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>http://www.thehindu.com</td>\n",
       "      <td>the-hindu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>http://www.huffingtonpost.com</td>\n",
       "      <td>the-huffington-post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://www.irishtimes.com</td>\n",
       "      <td>the-irish-times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://www.jpost.com/</td>\n",
       "      <td>the-jerusalem-post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>http://www.theladbible.com</td>\n",
       "      <td>the-lad-bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>http://www.nytimes.com</td>\n",
       "      <td>the-new-york-times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>http://thenextweb.com</td>\n",
       "      <td>the-next-web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>http://www.thesportbible.com</td>\n",
       "      <td>the-sport-bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>http://www.telegraph.co.uk</td>\n",
       "      <td>the-telegraph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>http://timesofindia.indiatimes.com</td>\n",
       "      <td>the-times-of-india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>http://www.theverge.com</td>\n",
       "      <td>the-verge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>http://www.wsj.com</td>\n",
       "      <td>the-wall-street-journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>https://www.washingtonpost.com</td>\n",
       "      <td>the-washington-post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.washingtontimes.com/</td>\n",
       "      <td>the-washington-times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>http://time.com</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>http://www.usatoday.com/news</td>\n",
       "      <td>usa-today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>https://news.vice.com</td>\n",
       "      <td>vice-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>https://www.wired.com</td>\n",
       "      <td>wired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>https://www.wired.de</td>\n",
       "      <td>wired-de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>http://www.wiwo.de</td>\n",
       "      <td>wirtschafts-woche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>http://xinhuanet.com/</td>\n",
       "      <td>xinhua-net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>http://www.ynet.co.il</td>\n",
       "      <td>ynet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        domains                      sources\n",
       "0                        https://abcnews.go.com                     abc-news\n",
       "1                    http://www.abc.net.au/news                  abc-news-au\n",
       "2                    https://www.aftenposten.no                  aftenposten\n",
       "3                      http://www.aljazeera.com           al-jazeera-english\n",
       "4                            http://www.ansa.it                         ansa\n",
       "5                         http://www.argaam.com                       argaam\n",
       "6                        http://arstechnica.com                 ars-technica\n",
       "7                        https://arynews.tv/ud/                     ary-news\n",
       "8                           https://apnews.com/             associated-press\n",
       "9                            http://www.afr.com  australian-financial-review\n",
       "10                        https://www.axios.com                        axios\n",
       "11                    http://www.bbc.co.uk/news                     bbc-news\n",
       "12                   http://www.bbc.co.uk/sport                    bbc-sport\n",
       "13                           http://www.bild.de                         bild\n",
       "14                   http://br.blastingnews.com             blasting-news-br\n",
       "15                http://www.bleacherreport.com              bleacher-report\n",
       "16                     http://www.bloomberg.com                    bloomberg\n",
       "17                     http://www.breitbart.com               breitbart-news\n",
       "18               http://www.businessinsider.com             business-insider\n",
       "19                http://uk.businessinsider.com          business-insider-uk\n",
       "20                     https://www.buzzfeed.com                     buzzfeed\n",
       "21                       http://www.cbc.ca/news                     cbc-news\n",
       "22                       http://www.cbsnews.com                     cbs-news\n",
       "23                          http://www.cnbc.com                         cnbc\n",
       "24                            http://us.cnn.com                          cnn\n",
       "25                   http://cnnespanol.cnn.com/                       cnn-es\n",
       "26                          https://www.ccn.com            crypto-coins-news\n",
       "27   http://www.dailymail.co.uk/home/index.html                   daily-mail\n",
       "28                   http://www.tagesspiegel.de             der-tagesspiegel\n",
       "29                     http://www.zeit.de/index                     die-zeit\n",
       "..                                          ...                          ...\n",
       "104                               http://t3n.de                          t3n\n",
       "105                        http://talksport.com                    talksport\n",
       "106                      https://techcrunch.com                   techcrunch\n",
       "107                       https://techcrunch.cn                techcrunch-cn\n",
       "108                    http://www.techradar.com                    techradar\n",
       "109     http://www.theamericanconservative.com/    the-american-conservative\n",
       "110             https://www.theglobeandmail.com           the-globe-and-mail\n",
       "111                          http://thehill.com                     the-hill\n",
       "112                     http://www.thehindu.com                    the-hindu\n",
       "113               http://www.huffingtonpost.com          the-huffington-post\n",
       "114                  https://www.irishtimes.com              the-irish-times\n",
       "115                      https://www.jpost.com/           the-jerusalem-post\n",
       "116                  http://www.theladbible.com                the-lad-bible\n",
       "117                      http://www.nytimes.com           the-new-york-times\n",
       "118                       http://thenextweb.com                 the-next-web\n",
       "119                http://www.thesportbible.com              the-sport-bible\n",
       "120                  http://www.telegraph.co.uk                the-telegraph\n",
       "121          http://timesofindia.indiatimes.com           the-times-of-india\n",
       "122                     http://www.theverge.com                    the-verge\n",
       "123                          http://www.wsj.com      the-wall-street-journal\n",
       "124              https://www.washingtonpost.com          the-washington-post\n",
       "125            https://www.washingtontimes.com/         the-washington-times\n",
       "126                             http://time.com                         time\n",
       "127                http://www.usatoday.com/news                    usa-today\n",
       "128                       https://news.vice.com                    vice-news\n",
       "129                       https://www.wired.com                        wired\n",
       "130                        https://www.wired.de                     wired-de\n",
       "131                          http://www.wiwo.de            wirtschafts-woche\n",
       "132                       http://xinhuanet.com/                   xinhua-net\n",
       "133                       http://www.ynet.co.il                         ynet\n",
       "\n",
       "[134 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "sources = newsapi.get_sources()\n",
    "new_orgs = sources[\"sources\"]\n",
    "my_sources = {}\n",
    "for i, x in enumerate(new_orgs):\n",
    "    my_sources[i] = (x['id'])\n",
    "domains = sources[\"sources\"]\n",
    "my_domains = {}\n",
    "for i, x in enumerate(domains):\n",
    "    my_domains[i] = (x['url'])\n",
    "sources = pd.Series(my_sources).to_frame(\"sources\")\n",
    "domains = pd.Series(my_domains).to_frame(\"domains\")\n",
    "query_keys_df = domains.join(sources)\n",
    "(query_keys_df)\n",
    "#query_keys_df.to_csv('lists.csv')\n",
    "#3,16,17,24,40,77,124,123,18,117,81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing data sources\n",
    "+ Lets attempt to grab some sources from different geographic locations as well as different idological perspectives\n",
    "\n",
    "+ categorizing news sources\n",
    "    + Traditional TV MSM\n",
    "        +  http://us.cnn.com   \n",
    "        +  http://www.cnbc.com \n",
    "        +  http://www.foxnews.com  \n",
    "        +  http://www.msnbc.com  \n",
    "        +  https://abcnews.go.com  \n",
    "        +  http://www.nbcnews.com  \n",
    "    + Traditional publications \n",
    "        +  http://www.nytimes.com  \n",
    "        +  https://www.washingtonpost.com \n",
    "        \n",
    "    + Internet Sources\n",
    "        +  http://www.huffingtonpost.com \n",
    "        +  https://www.politico.com\n",
    "        +  http://www.breitbart.com \n",
    "        +  https://news.google.com \n",
    "        +  https://www.buzzfeed.com \n",
    "        +  https://news.vice.com  \n",
    "    + Financial publications\n",
    "        +  http://www.economist.com\n",
    "        +  http://www.bloomberg.com \n",
    "        +  http://www.businessinsider.com \n",
    "        +  http://www.wsj.com\n",
    "        +  http://fortune.com  \n",
    "        \n",
    "    + News aggregators\n",
    "        +  https://apnews.com/ \n",
    "        +  http://www.reuters.com \n",
    "    + foreign reporting\n",
    "         + http://www.aljazeera.com  \n",
    "         + http://www.bbc.co.uk/news   \n",
    "         + https://www.jpost.com/  \n",
    "         + http://timesofindia.indiatimes.com \n",
    "         + https://russian.rt.com \n",
    "         + https://www.theguardian.com/uk \n",
    "         + http://www.independent.co.uk  \n",
    "         + http://www.telegraph.co.uk  \n",
    "\n",
    "\n",
    " \n",
    "### Build Query String "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fox-news,google-news,msnbc'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Literally picking data sources from df i printed above \n",
    "a= query_keys_df.iloc[[16,17,24],[1]]\n",
    "b= query_keys_df.iloc[[40,43,77],[1]]\n",
    "list_sources =a[\"sources\"].tolist()\n",
    "\n",
    "## build out string for query request \n",
    "myString = \",\".join(list_sources)\n",
    "\n",
    "list_sources2 = b[\"sources\"].tolist()\n",
    "myString2 = \",\".join(list_sources2)\n",
    "myString2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets begin process of automating query calls\n",
    "+ first lets build function to clean query returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(query):\n",
    "    for x in query['articles']:\n",
    "        try:\n",
    "            x[\"source\"] = x[\"source\"][\"name\"]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            x['publishedAt'] = str.split(x['publishedAt'], \"T\")[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del x['urlToImage']\n",
    "        except KeyError:\n",
    "            pass\n",
    "    my_df = pd.DataFrame(query[\"articles\"])\n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to hit the api\n",
    "+ Originally I had a loop here.  instead I figured I would just build a function that takes start data, end data, query term(candidate)\n",
    "    + The original code kept giving me a query limit reached result, so I decided to change up strategy and search one day at a time\n",
    "    + After we hit the papers at the start for past 30 days, we will only need 1 day at a time going forward.\n",
    "    + I built in some print statements for error handeling, which you will see below in the block after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "candidates_list=[]\n",
    "total_count=[]\n",
    "\n",
    "# Make first call\n",
    "def hit_api(start,end,q,myString):\n",
    "    \n",
    "    ## catch bug with formatted strings for dates\n",
    "    if end < 10:\n",
    "        start_str = \"0\"+ str(start)\n",
    "        end_str = \"0\"+ str(end)\n",
    "    elif end==10:\n",
    "        start_str = \"0\"+ str(start)\n",
    "        end_str = str(end)\n",
    "    else :\n",
    "        start_str = str(start)\n",
    "        end_str = str(end)\n",
    "      \n",
    "    ## API query\n",
    "    all_articles = newsapi.get_everything(q=q,\n",
    "                                          sources=myString,\n",
    "                                          language='en',\n",
    "                                          from_param='2019-09-{}'.format(start_str),\n",
    "                                          to='2019-09-{}'.format(end_str),\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=100,\n",
    "                                          page=1)\n",
    "    \n",
    "    ## get count\n",
    "    total_pages = math.ceil(all_articles[\"totalResults\"]/100)\n",
    "    print(\"query will return: \"+ str(all_articles[\"totalResults\"]))\n",
    "    \n",
    "    ## store count to check versus dimension of df later\n",
    "    total_count.append(all_articles[\"totalResults\"])\n",
    "    \n",
    "    ## Clen query \n",
    "    all_articles = clean_query(all_articles)\n",
    "    \n",
    "    ## append to list\n",
    "    candidates_list.append(all_articles)\n",
    "    ##validate that df shape[0] is equal to expected query count\n",
    "    print(\"we should expect: {} articles\".format(sum(total_count)))\n",
    "    print(\"we have: {} articles\".format(Bernie_df.shape[0]))\n",
    "\n",
    "\n",
    "    ## test 100 results theory\n",
    "    over_100 = sum([x-100 for x in total_count if x>100 ])\n",
    "    print(\"{} results came into query that exceeded 100 hits in a day.  Thus we lose any hit over 100 in a day\".format(over_100))\n",
    "\n",
    "\n",
    "    ##unique links\n",
    "    unique_array = Bernie_df.url.unique()\n",
    "    print(\"we have {} unique links\".format(unique_array.shape[0]))\n",
    "\n",
    "    return(candidates_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built out a loop \n",
    "+ Simple, look at first day of September to last day incrementing start and end by 1 each time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(20,21):\n",
    "    df=hit_api(x,x+1,'Warren OR Elizabeth Warren',myString)\n",
    "    \n",
    "## collapse list on itself to build big df\n",
    "Bernie_df = pd.concat(df)\n",
    "Bernie_df= Bernie_df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sanity checks\n",
    "\n",
    "### Issues\n",
    "+ The counts don't match up between expected and actual because the query results are limited to first 100 hits\n",
    "    + Won't be an issue with less publications, but we need to be careful splitting candidate calls up.\n",
    "+ Not all links are unique\n",
    "    + This is expected given internet sources reposting articles from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we should expect: 576 articles\n",
      "we have: 333 articles\n",
      "243 results came into query that exceeded 100 hits in a day.  Thus we lose any hit over 100 in a day\n",
      "we have 224 unique links\n"
     ]
    }
   ],
   "source": [
    "##validate that df shape[0] is equal to expected query count\n",
    "print(\"we should expect: {} articles\".format(sum(total_count)))\n",
    "print(\"we have: {} articles\".format(Bernie_df.shape[0]))\n",
    "\n",
    "\n",
    "## test 100 results theory\n",
    "over_100 = sum([x-100 for x in total_count if x>100 ])\n",
    "print(\"{} results came into query that exceeded 100 hits in a day.  Thus we lose any hit over 100 in a day\".format(over_100))\n",
    "\n",
    "\n",
    "##unique links\n",
    "unique_array = Bernie_df.url.unique()\n",
    "print(\"we have {} unique links\".format(unique_array.shape[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicateRowsDF = Bernie_df[Bernie_df.duplicated(\"url\")]\n",
    "duplicateRowsDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bernie_df.sort_values(by=['url'])\n",
    "Bernie_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hitting newspaper 3k Api with links\n",
    "+ We can deal with duplicates and streamlining the above later\n",
    "+ to demonstrate a working product I feed our url into newspaper 3k \n",
    "+ Id say it takes 5-10 seconds per article to fetch complete text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}\n",
    "\n",
    "counts=0\n",
    "list_full_text=[]\n",
    "for link in Bernie_df['url']:\n",
    "    counts+=1\n",
    "    print(counts)\n",
    "    html = requests.get(link,headers=headers).text\n",
    "    try: text = fulltext(html)\n",
    "    except: \n",
    "        print(\"no words found\")\n",
    "        text=\"no words found\"\n",
    "        list_full_text.append(text)\n",
    "        continue\n",
    "    list_full_text.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernie_df[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add text to previous dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernie_df['full_art']=pd.Series(list_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     One of former Vice President Joe Biden’s top d...\n",
       "1     Cedar Rapids, Iowa (CNN) Sen. Elizabeth Warren...\n",
       "2     Sarah Isgur is a CNN political analyst. She ha...\n",
       "3     Despite 2020 Democrat presidential primary can...\n",
       "4     (CNN) Democratic Rep. Joe Kennedy III of Massa...\n",
       "5     (CNN) House Speaker Nancy Pelosi slammed the T...\n",
       "6     (CNN) Billionaire Robert Smith has made good o...\n",
       "7     Sen. Elizabeth Warren (D-MA) is rising in Cali...\n",
       "8     Senator Chris Coons (D-DE) on Friday discussed...\n",
       "9     Sen. Bernie Sanders (I-VT) responded to a lett...\n",
       "10    Rep. Joe Kennedy III (D-MA) formally announced...\n",
       "11    Sen. Bernie Sanders (I-VT) is joining striking...\n",
       "12    Presidential candidate Andrew Yang (D) has the...\n",
       "13    Several 2020 Democrats expressed support for s...\n",
       "14    Sen. Cory Booker (D-NJ) told HuffPost in an in...\n",
       "15    Activists across over 150 countries are partic...\n",
       "16    Several hundred Tea Party activists gathered T...\n",
       "17    Sen. Kamala Harris (D-CA) is beefing up resour...\n",
       "18    Dean Obeidallah, a former attorney, is the hos...\n",
       "19    Chat with us in Facebook Messenger. Find out w...\n",
       "Name: full_art, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bernie_df[:20]['full_art']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "Bernie_df.to_csv('data_'+str(time.time())+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "+ we need to functionize streamline and clean up query calls.\n",
    "+ sorry my python is rusty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1571550604.3020012"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
