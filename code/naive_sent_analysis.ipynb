{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "from nltk import sent_tokenize\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from afinn import Afinn\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Biden': Counter(), 'Bernie': Counter(), 'Warren': Counter(), 'Kamala': Counter()}\n",
      "{'Biden': Counter(), 'Bernie': Counter(), 'Warren': Counter(), 'Kamala': Counter()}\n",
      "{'Biden': Counter({'neg': 546, 'pos': 541}), 'Bernie': Counter({'neg': 176, 'pos': 142}), 'Warren': Counter({'pos': 165, 'neg': 163}), 'Kamala': Counter({'pos': 81, 'neg': 80})}\n",
      "{'Biden': Counter({'neg': 546, 'pos': 541}), 'Bernie': Counter({'neg': 176, 'pos': 142}), 'Warren': Counter({'pos': 165, 'neg': 163}), 'Kamala': Counter({'pos': 81, 'neg': 80})}\n",
      "{'Biden': Counter({'neg': 579, 'pos': 509}), 'Bernie': Counter({'neg': 208, 'pos': 180}), 'Warren': Counter({'neg': 318, 'pos': 300}), 'Kamala': Counter({'neg': 99, 'pos': 78})}\n",
      "{'Biden': Counter({'neg': 579, 'pos': 509}), 'Bernie': Counter({'neg': 208, 'pos': 180}), 'Warren': Counter({'neg': 318, 'pos': 300}), 'Kamala': Counter({'neg': 99, 'pos': 78})}\n",
      "{'Biden': Counter({'pos': 894, 'neg': 836}), 'Bernie': Counter({'pos': 165, 'neg': 163}), 'Warren': Counter({'neg': 308, 'pos': 292}), 'Kamala': Counter({'neg': 135, 'pos': 122})}\n",
      "{'Biden': Counter({'pos': 894, 'neg': 836}), 'Bernie': Counter({'pos': 165, 'neg': 163}), 'Warren': Counter({'neg': 308, 'pos': 292}), 'Kamala': Counter({'neg': 135, 'pos': 122})}\n",
      "{'Biden': Counter({'neg': 16, 'pos': 8}), 'Bernie': Counter({'neg': 24, 'pos': 17}), 'Warren': Counter({'neg': 31, 'pos': 17}), 'Kamala': Counter({'neg': 4, 'pos': 4})}\n",
      "{'Biden': Counter({'neg': 16, 'pos': 8}), 'Bernie': Counter({'neg': 24, 'pos': 17}), 'Warren': Counter({'neg': 31, 'pos': 17}), 'Kamala': Counter({'neg': 4, 'pos': 4})}\n",
      "{'Biden': Counter({'pos': 99, 'neg': 93}), 'Bernie': Counter({'neg': 31, 'pos': 27}), 'Warren': Counter({'neg': 31, 'pos': 22}), 'Kamala': Counter({'pos': 11, 'neg': 7})}\n",
      "{'Biden': Counter({'pos': 99, 'neg': 93}), 'Bernie': Counter({'neg': 31, 'pos': 27}), 'Warren': Counter({'neg': 31, 'pos': 22}), 'Kamala': Counter({'pos': 11, 'neg': 7})}\n",
      "{'Biden': Counter({'neg': 296, 'pos': 281}), 'Bernie': Counter({'pos': 174, 'neg': 167}), 'Warren': Counter({'pos': 217, 'neg': 202}), 'Kamala': Counter({'neg': 52, 'pos': 42})}\n",
      "{'Biden': Counter({'neg': 296, 'pos': 281}), 'Bernie': Counter({'pos': 174, 'neg': 167}), 'Warren': Counter({'pos': 217, 'neg': 202}), 'Kamala': Counter({'neg': 52, 'pos': 42})}\n",
      "{'Biden': Counter({'pos': 240, 'neg': 144}), 'Bernie': Counter({'neg': 103, 'pos': 91}), 'Warren': Counter({'pos': 133, 'neg': 114}), 'Kamala': Counter({'neg': 23, 'pos': 21})}\n",
      "{'Biden': Counter({'pos': 240, 'neg': 144}), 'Bernie': Counter({'neg': 103, 'pos': 91}), 'Warren': Counter({'pos': 133, 'neg': 114}), 'Kamala': Counter({'neg': 23, 'pos': 21})}\n",
      "{'Biden': Counter({'pos': 26, 'neg': 17}), 'Bernie': Counter({'pos': 8, 'neg': 2}), 'Warren': Counter({'neg': 11, 'pos': 11}), 'Kamala': Counter({'neg': 3, 'pos': 3})}\n",
      "{'Biden': Counter({'pos': 26, 'neg': 17}), 'Bernie': Counter({'pos': 8, 'neg': 2}), 'Warren': Counter({'neg': 11, 'pos': 11}), 'Kamala': Counter({'neg': 3, 'pos': 3})}\n",
      "{'Biden': Counter({'neg': 1142, 'pos': 1068}), 'Bernie': Counter({'neg': 335, 'pos': 286}), 'Warren': Counter({'neg': 513, 'pos': 407}), 'Kamala': Counter({'neg': 170, 'pos': 147})}\n",
      "{'Biden': Counter({'neg': 1142, 'pos': 1068}), 'Bernie': Counter({'neg': 335, 'pos': 286}), 'Warren': Counter({'neg': 513, 'pos': 407}), 'Kamala': Counter({'neg': 170, 'pos': 147})}\n"
     ]
    }
   ],
   "source": [
    "import s1_analysis\n",
    "import collections\n",
    "s1 = s1_analysis.s1_analysis()\n",
    "s1.train_load(\"s1_model_2.sav\")\n",
    "from re import search\n",
    "pubs=[]\n",
    "names_list=[\"Biden\",\"Bernie\",\"Warren\",\"Kamala\"]\n",
    "## read in data\n",
    "df=pd.read_csv('vikas.csv')\n",
    "df['tokenized_sents'] = df.apply(lambda row: nltk.sent_tokenize(row['full_art']), axis=1)\n",
    "\n",
    "## split soruces into dictionary keys based on their values\n",
    "sources={}\n",
    "for source, df_source in df.groupby('source'):\n",
    "    sources[source]=df_source\n",
    "\n",
    "    \n",
    "    \n",
    "## grabs all snetences which mention candidate\n",
    "def get_candidate_mentions(candidate_name,publisher):\n",
    "    mentions=[]\n",
    "    substring=candidate_name\n",
    "    for article in sources[publisher]['tokenized_sents']:\n",
    "        for sentence in article:\n",
    "            if search(substring, sentence):\n",
    "                mentions.append(sentence)\n",
    "            else:\n",
    "                continue\n",
    "   # print(len(mentions))\n",
    "    return(mentions)\n",
    "\n",
    "## calls functiom that builds candidate df's and runs sentiment over those df returning ratio of pos:neg\n",
    "def get_sentiment_scores(names_list,publisher):\n",
    "    ## build df's\n",
    "    cand_sent={}\n",
    "    joe_biden_mentions=get_candidate_mentions(\"Biden|Joe Biden\",publisher)\n",
    "    bernie_mentions=get_candidate_mentions(\"Bernie|Sanders\",publisher)\n",
    "    Warren_mentions=get_candidate_mentions(\"Warren|Elizabeth Warren\",publisher)\n",
    "    Kamala_mentions=get_candidate_mentions(\"Kamala|Harris\",publisher)\n",
    "    #print(len(joe_biden_mentions))\n",
    "    ## build lists for sentiment scoring loop\n",
    "    cand_list=[joe_biden_mentions,bernie_mentions,Warren_mentions,Kamala_mentions]\n",
    "\n",
    "    ## run sentiment score on each candidate and get count store in dictionary\n",
    "    for i,candidate in enumerate(cand_list):\n",
    "        features=[]\n",
    "        results=[]\n",
    "        features = [s1.find_features(x) for x in candidate]\n",
    "        results = [s1.classifier.classify(x) for x in features ]\n",
    "        counter=collections.Counter(results)\n",
    "        cand_sent[names_list[i]]=counter\n",
    "    print(cand_sent)\n",
    "    return(cand_sent)\n",
    "\n",
    "\n",
    "\n",
    "## run over all publications\n",
    "## empty dict to store all results\n",
    "all_publishers={}\n",
    "pubnames=[x for x in sources.keys()]\n",
    "for i,publisher in enumerate(pubnames):\n",
    "   # pubs.append(get_sentiment_scores(names_list,publisher))\n",
    "    scores=get_sentiment_scores(names_list,publisher)\n",
    "    print(scores)\n",
    "    all_publishers[publisher]=scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bloomberg': {'Biden': Counter(),\n",
       "  'Bernie': Counter(),\n",
       "  'Warren': Counter(),\n",
       "  'Kamala': Counter()},\n",
       " 'Breitbart News': {'Biden': Counter({'neg': 546, 'pos': 541}),\n",
       "  'Bernie': Counter({'neg': 176, 'pos': 142}),\n",
       "  'Warren': Counter({'neg': 163, 'pos': 165}),\n",
       "  'Kamala': Counter({'pos': 81, 'neg': 80})},\n",
       " 'CNN': {'Biden': Counter({'pos': 509, 'neg': 579}),\n",
       "  'Bernie': Counter({'neg': 208, 'pos': 180}),\n",
       "  'Warren': Counter({'pos': 300, 'neg': 318}),\n",
       "  'Kamala': Counter({'neg': 99, 'pos': 78})},\n",
       " 'Fox News': {'Biden': Counter({'neg': 836, 'pos': 894}),\n",
       "  'Bernie': Counter({'pos': 165, 'neg': 163}),\n",
       "  'Warren': Counter({'pos': 292, 'neg': 308}),\n",
       "  'Kamala': Counter({'neg': 135, 'pos': 122})},\n",
       " 'Google News': {'Biden': Counter({'pos': 8, 'neg': 16}),\n",
       "  'Bernie': Counter({'neg': 24, 'pos': 17}),\n",
       "  'Warren': Counter({'neg': 31, 'pos': 17}),\n",
       "  'Kamala': Counter({'neg': 4, 'pos': 4})},\n",
       " 'MSNBC': {'Biden': Counter({'pos': 99, 'neg': 93}),\n",
       "  'Bernie': Counter({'pos': 27, 'neg': 31}),\n",
       "  'Warren': Counter({'neg': 31, 'pos': 22}),\n",
       "  'Kamala': Counter({'pos': 11, 'neg': 7})},\n",
       " 'New York Magazine': {'Biden': Counter({'neg': 296, 'pos': 281}),\n",
       "  'Bernie': Counter({'neg': 167, 'pos': 174}),\n",
       "  'Warren': Counter({'neg': 202, 'pos': 217}),\n",
       "  'Kamala': Counter({'neg': 52, 'pos': 42})},\n",
       " 'The New York Times': {'Biden': Counter({'pos': 240, 'neg': 144}),\n",
       "  'Bernie': Counter({'neg': 103, 'pos': 91}),\n",
       "  'Warren': Counter({'pos': 133, 'neg': 114}),\n",
       "  'Kamala': Counter({'neg': 23, 'pos': 21})},\n",
       " 'The Wall Street Journal': {'Biden': Counter({'pos': 26, 'neg': 17}),\n",
       "  'Bernie': Counter({'pos': 8, 'neg': 2}),\n",
       "  'Warren': Counter({'neg': 11, 'pos': 11}),\n",
       "  'Kamala': Counter({'neg': 3, 'pos': 3})},\n",
       " 'The Washington Post': {'Biden': Counter({'neg': 1142, 'pos': 1068}),\n",
       "  'Bernie': Counter({'pos': 286, 'neg': 335}),\n",
       "  'Warren': Counter({'neg': 513, 'pos': 407}),\n",
       "  'Kamala': Counter({'neg': 170, 'pos': 147})}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_publishers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Extra code from previous project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "def name_entity_recognition(sentence):\n",
    "    '''\n",
    "    A function to retrieve name entities in a sentence.\n",
    "    :param sentence: the sentence to retrieve names from.\n",
    "    :return: a name entity list of the sentence.\n",
    "    '''\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    # retrieve person and organization's name from the sentence\n",
    "    name_entity = [x for x in doc.ents if x.label_ in ['PERSON']]\n",
    "    # convert all names to lowercase and remove 's and ’s in names\n",
    "    name_entity = [str(x).lower().replace(\"'s\", \"\") for x in name_entity]\n",
    "    name_entity = [x.replace(\"’s\", \"\") for x in name_entity]\n",
    "    # remove name words that are less than 3 letters to raise recognition accuracy\n",
    "    name_entity = [x for x in name_entity if len(x) >= 3]\n",
    "\n",
    "    return name_entity\n",
    "\n",
    "def flatten(l):\n",
    "    \"\"\"A function that flattens a complex list\"\"\"\n",
    "    flat_list = []\n",
    "    for i in l:\n",
    "        for j in i:\n",
    "            flat_list.append(j)\n",
    "    return flat_list\n",
    "def nlist(book):\n",
    "    \"\"\"Returns a unique list of names from a sentence tokenized book\"\"\"\n",
    "    names = []\n",
    "    for i in book:\n",
    "        if name_entity_recognition(i) != []:\n",
    "            names.append(name_entity_recognition(i))\n",
    "    names = list(set(flatten(names)))\n",
    "    return names\n",
    "\n",
    "\n",
    "def top_names(name_list, novel, top_num=25):\n",
    "    '''\n",
    "    Returns name freq of a book for each name\n",
    "    '''\n",
    "\n",
    "    vect = CountVectorizer(vocabulary=name_list, stop_words='english')\n",
    "    name_frequency = vect.fit_transform([novel.lower()])\n",
    "    name_frequency = pd.DataFrame(\n",
    "        name_frequency.toarray(), columns=vect.get_feature_names())\n",
    "    name_frequency = name_frequency.T\n",
    "    name_frequency = name_frequency.sort_values(by=0, ascending=False)\n",
    "    name_frequency = name_frequency[0:top_num]\n",
    "    names = list(name_frequency.index)\n",
    "    name_frequency = list(name_frequency[0])\n",
    "\n",
    "    return name_frequency, names\n",
    "def name_freq_plot(df, title):\n",
    "    \"\"\"plot for name freq\"\"\"\n",
    "    sns.barplot(data=df,\n",
    "                y=df.names,\n",
    "                x=df.freq,\n",
    "                color='blue')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds a dictionary which stores all the names used in all articles from CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this will build a dictionary which stores all the names used in all articles from CNN\n",
    "sources[\"CNN\"]['tokenized_sents'] = sources[\"CNN\"].apply(lambda row: nltk.sent_tokenize(row['full_art']), axis=1)\n",
    "article_dict={}\n",
    "for i,x in enumerate(sources[\"CNN\"]['tokenized_sents']):\n",
    "    article_dict[i]=nlist(x)\n",
    "    print(i)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "638.212px",
    "left": "1326.79px",
    "right": "20px",
    "top": "120px",
    "width": "248.767px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
