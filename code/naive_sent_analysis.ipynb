{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "from nltk import sent_tokenize\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from afinn import Afinn\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search\n",
    "import s1_analysis\n",
    "import collections\n",
    "s1 = s1_analysis.s1_analysis()\n",
    "s1.train_load(\"s1_model_2.sav\")\n",
    "pubs = []\n",
    "names_list = [\"Biden\", \"Bernie\", \"Warren\", \"Kamala\"]\n",
    "\n",
    "\n",
    "# read in data and sentence tokenize it\n",
    "df = pd.read_csv('vikas.csv')\n",
    "df['tokenized_sents'] = df.apply(\n",
    "    lambda row: nltk.sent_tokenize(row['full_art']), axis=1)\n",
    "\n",
    "\n",
    "# split df into dictionary with keys representing publisher and values representing df for those publishers\n",
    "sources = {}\n",
    "for source, df_source in df.groupby('source'):\n",
    "    sources[source] = df_source\n",
    "\n",
    "# grabs all sentences which mention candidate\n",
    "\n",
    "\n",
    "def get_candidate_mentions(candidate_name, publisher, tokenize_level):\n",
    "    mentions = []\n",
    "    substring = candidate_name\n",
    "    for article in sources[publisher][tokenize_level]:\n",
    "        for sentence in article:\n",
    "            if search(substring, sentence):\n",
    "                mentions.append(sentence)\n",
    "            else:\n",
    "                continue\n",
    "    return(mentions)\n",
    "\n",
    "\n",
    "def get_sentiment_scores(names_list, publisher, tokenize_level, no_duplicates=None):\n",
    "   # will store cand name and sentiment scores\n",
    "    cand_sent = {}\n",
    "\n",
    "    # build cand mention lists\n",
    "    joe_biden_mentions = get_candidate_mentions(\n",
    "        \"Biden|Joe Biden\", publisher, tokenize_level)\n",
    "    bernie_mentions = get_candidate_mentions(\n",
    "        \"Bernie|Sanders\", publisher, tokenize_level)\n",
    "    Warren_mentions = get_candidate_mentions(\n",
    "        \"Warren|Elizabeth Warren\", publisher, tokenize_level)\n",
    "    Kamala_mentions = get_candidate_mentions(\n",
    "        \"Kamala|Harris\", publisher, tokenize_level)\n",
    "\n",
    "    # build lists for sentiment scoring loop\n",
    "    cand_list = [joe_biden_mentions, bernie_mentions,\n",
    "                 Warren_mentions, Kamala_mentions]\n",
    "\n",
    "    # If we want to eliminate sentences with multiple candididate mentions, we call fucntion\n",
    "    # with no_duplicates=1 and this block executes\n",
    "\n",
    "    if no_duplicates == 1:\n",
    "\n",
    "        # shared_sent will grab any shared sentences\n",
    "        shared_sent = []\n",
    "        for x in joe_biden_mentions:\n",
    "            if search(\"Bernie|Sanders|Warren|Elizabeth Warren|Kamala|Harris\", x):\n",
    "                shared_sent.append(x)\n",
    "        for x in bernie_mentions:\n",
    "            if search(\"Joe|Joe Biden|Warren|Elizabeth Warren|Kamala|Harris\", x):\n",
    "                shared_sent.append(x)\n",
    "        for x in Warren_mentions:\n",
    "            if search(\"Joe|Joe Biden|Bernie|Sanders|Kamala|Harris\", x):\n",
    "                shared_sent.append(x)\n",
    "        for x in Kamala_mentions:\n",
    "            if search(\"Joe|Joe Biden|Bernie|Sanders|Warren|Elizabeth Warren\", x):\n",
    "                shared_sent.append(x)\n",
    "\n",
    "        # use shared sent list to remove shared sent from each candidates sentence list\n",
    "        for cand in cand_list:\n",
    "            for sent in cand:\n",
    "                if sent in shared_sent:\n",
    "                    cand.remove(sent)\n",
    "\n",
    "    # run sentiment score on each candidate and get count/store in returned dictionary\n",
    "    for i, candidate in enumerate(cand_list):\n",
    "        features = []\n",
    "        results = []\n",
    "        features = [s1.find_features(x) for x in candidate]\n",
    "        results = [s1.classifier.classify(x) for x in features]\n",
    "        counter = collections.Counter(results)\n",
    "        cand_sent[names_list[i]] = counter\n",
    "    ##print(cand_sent)\n",
    "    return(cand_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Biden': Counter(), 'Bernie': Counter(), 'Warren': Counter(), 'Kamala': Counter()}\n",
      "{'Biden': Counter(), 'Bernie': Counter(), 'Warren': Counter(), 'Kamala': Counter()}\n",
      "{'Biden': Counter({'neg': 546, 'pos': 541}), 'Bernie': Counter({'neg': 176, 'pos': 142}), 'Warren': Counter({'pos': 165, 'neg': 163}), 'Kamala': Counter({'pos': 81, 'neg': 80})}\n",
      "{'Biden': Counter({'neg': 546, 'pos': 541}), 'Bernie': Counter({'neg': 176, 'pos': 142}), 'Warren': Counter({'pos': 165, 'neg': 163}), 'Kamala': Counter({'pos': 81, 'neg': 80})}\n",
      "{'Biden': Counter({'neg': 579, 'pos': 509}), 'Bernie': Counter({'neg': 208, 'pos': 180}), 'Warren': Counter({'neg': 318, 'pos': 300}), 'Kamala': Counter({'neg': 99, 'pos': 78})}\n",
      "{'Biden': Counter({'neg': 579, 'pos': 509}), 'Bernie': Counter({'neg': 208, 'pos': 180}), 'Warren': Counter({'neg': 318, 'pos': 300}), 'Kamala': Counter({'neg': 99, 'pos': 78})}\n",
      "{'Biden': Counter({'pos': 894, 'neg': 836}), 'Bernie': Counter({'pos': 165, 'neg': 163}), 'Warren': Counter({'neg': 308, 'pos': 292}), 'Kamala': Counter({'neg': 135, 'pos': 122})}\n",
      "{'Biden': Counter({'pos': 894, 'neg': 836}), 'Bernie': Counter({'pos': 165, 'neg': 163}), 'Warren': Counter({'neg': 308, 'pos': 292}), 'Kamala': Counter({'neg': 135, 'pos': 122})}\n",
      "{'Biden': Counter({'neg': 16, 'pos': 8}), 'Bernie': Counter({'neg': 24, 'pos': 17}), 'Warren': Counter({'neg': 31, 'pos': 17}), 'Kamala': Counter({'neg': 4, 'pos': 4})}\n",
      "{'Biden': Counter({'neg': 16, 'pos': 8}), 'Bernie': Counter({'neg': 24, 'pos': 17}), 'Warren': Counter({'neg': 31, 'pos': 17}), 'Kamala': Counter({'neg': 4, 'pos': 4})}\n",
      "{'Biden': Counter({'pos': 99, 'neg': 93}), 'Bernie': Counter({'neg': 31, 'pos': 27}), 'Warren': Counter({'neg': 31, 'pos': 22}), 'Kamala': Counter({'pos': 11, 'neg': 7})}\n",
      "{'Biden': Counter({'pos': 99, 'neg': 93}), 'Bernie': Counter({'neg': 31, 'pos': 27}), 'Warren': Counter({'neg': 31, 'pos': 22}), 'Kamala': Counter({'pos': 11, 'neg': 7})}\n",
      "{'Biden': Counter({'neg': 296, 'pos': 281}), 'Bernie': Counter({'pos': 174, 'neg': 167}), 'Warren': Counter({'pos': 217, 'neg': 202}), 'Kamala': Counter({'neg': 52, 'pos': 42})}\n",
      "{'Biden': Counter({'neg': 296, 'pos': 281}), 'Bernie': Counter({'pos': 174, 'neg': 167}), 'Warren': Counter({'pos': 217, 'neg': 202}), 'Kamala': Counter({'neg': 52, 'pos': 42})}\n",
      "{'Biden': Counter({'pos': 240, 'neg': 144}), 'Bernie': Counter({'neg': 103, 'pos': 91}), 'Warren': Counter({'pos': 133, 'neg': 114}), 'Kamala': Counter({'neg': 23, 'pos': 21})}\n",
      "{'Biden': Counter({'pos': 240, 'neg': 144}), 'Bernie': Counter({'neg': 103, 'pos': 91}), 'Warren': Counter({'pos': 133, 'neg': 114}), 'Kamala': Counter({'neg': 23, 'pos': 21})}\n",
      "{'Biden': Counter({'pos': 26, 'neg': 17}), 'Bernie': Counter({'pos': 8, 'neg': 2}), 'Warren': Counter({'neg': 11, 'pos': 11}), 'Kamala': Counter({'neg': 3, 'pos': 3})}\n",
      "{'Biden': Counter({'pos': 26, 'neg': 17}), 'Bernie': Counter({'pos': 8, 'neg': 2}), 'Warren': Counter({'neg': 11, 'pos': 11}), 'Kamala': Counter({'neg': 3, 'pos': 3})}\n",
      "{'Biden': Counter({'neg': 1142, 'pos': 1068}), 'Bernie': Counter({'neg': 335, 'pos': 286}), 'Warren': Counter({'neg': 513, 'pos': 407}), 'Kamala': Counter({'neg': 170, 'pos': 147})}\n",
      "{'Biden': Counter({'neg': 1142, 'pos': 1068}), 'Bernie': Counter({'neg': 335, 'pos': 286}), 'Warren': Counter({'neg': 513, 'pos': 407}), 'Kamala': Counter({'neg': 170, 'pos': 147})}\n"
     ]
    }
   ],
   "source": [
    "# will return a dictionary that as a key gives candidate name and as a value\n",
    "# gives another dictionary with candidates name as key and sentiment scores as values\n",
    "all_publishers = {}\n",
    "pubnames = [x for x in sources.keys()]\n",
    "for i, publisher in enumerate(pubnames):\n",
    "   # pubs.append(get_sentiment_scores(names_list,publisher))\n",
    "    scores = get_sentiment_scores(names_list, publisher, 'tokenized_sents')\n",
    "    print(scores)\n",
    "    all_publishers[publisher] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Biden': Counter(), 'Bernie': Counter(), 'Warren': Counter(), 'Kamala': Counter()}\n",
      "{'Biden': Counter(), 'Bernie': Counter(), 'Warren': Counter(), 'Kamala': Counter()}\n",
      "{'Biden': Counter({'neg': 519, 'pos': 504}), 'Bernie': Counter({'neg': 143, 'pos': 110}), 'Warren': Counter({'neg': 137, 'pos': 119}), 'Kamala': Counter({'neg': 70, 'pos': 67})}\n",
      "{'Biden': Counter({'neg': 519, 'pos': 504}), 'Bernie': Counter({'neg': 143, 'pos': 110}), 'Warren': Counter({'neg': 137, 'pos': 119}), 'Kamala': Counter({'neg': 70, 'pos': 67})}\n",
      "{'Biden': Counter({'neg': 532, 'pos': 446}), 'Bernie': Counter({'neg': 170, 'pos': 136}), 'Warren': Counter({'neg': 261, 'pos': 221}), 'Kamala': Counter({'neg': 81, 'pos': 62})}\n",
      "{'Biden': Counter({'neg': 532, 'pos': 446}), 'Bernie': Counter({'neg': 170, 'pos': 136}), 'Warren': Counter({'neg': 261, 'pos': 221}), 'Kamala': Counter({'neg': 81, 'pos': 62})}\n",
      "{'Biden': Counter({'pos': 809, 'neg': 797}), 'Bernie': Counter({'neg': 112, 'pos': 108}), 'Warren': Counter({'neg': 242, 'pos': 202}), 'Kamala': Counter({'neg': 116, 'pos': 100})}\n",
      "{'Biden': Counter({'pos': 809, 'neg': 797}), 'Bernie': Counter({'neg': 112, 'pos': 108}), 'Warren': Counter({'neg': 242, 'pos': 202}), 'Kamala': Counter({'neg': 116, 'pos': 100})}\n",
      "{'Biden': Counter({'neg': 10, 'pos': 7}), 'Bernie': Counter({'neg': 17, 'pos': 13}), 'Warren': Counter({'neg': 21, 'pos': 14}), 'Kamala': Counter({'pos': 4, 'neg': 1})}\n",
      "{'Biden': Counter({'neg': 10, 'pos': 7}), 'Bernie': Counter({'neg': 17, 'pos': 13}), 'Warren': Counter({'neg': 21, 'pos': 14}), 'Kamala': Counter({'pos': 4, 'neg': 1})}\n",
      "{'Biden': Counter({'pos': 88, 'neg': 84}), 'Bernie': Counter({'neg': 24, 'pos': 19}), 'Warren': Counter({'neg': 19, 'pos': 14}), 'Kamala': Counter({'pos': 9, 'neg': 5})}\n",
      "{'Biden': Counter({'pos': 88, 'neg': 84}), 'Bernie': Counter({'neg': 24, 'pos': 19}), 'Warren': Counter({'neg': 19, 'pos': 14}), 'Kamala': Counter({'pos': 9, 'neg': 5})}\n",
      "{'Biden': Counter({'neg': 260, 'pos': 220}), 'Bernie': Counter({'neg': 132, 'pos': 117}), 'Warren': Counter({'neg': 152, 'pos': 145}), 'Kamala': Counter({'neg': 38, 'pos': 25})}\n",
      "{'Biden': Counter({'neg': 260, 'pos': 220}), 'Bernie': Counter({'neg': 132, 'pos': 117}), 'Warren': Counter({'neg': 152, 'pos': 145}), 'Kamala': Counter({'neg': 38, 'pos': 25})}\n",
      "{'Biden': Counter({'pos': 205, 'neg': 128}), 'Bernie': Counter({'neg': 83, 'pos': 67}), 'Warren': Counter({'pos': 100, 'neg': 88}), 'Kamala': Counter({'neg': 19, 'pos': 15})}\n",
      "{'Biden': Counter({'pos': 205, 'neg': 128}), 'Bernie': Counter({'neg': 83, 'pos': 67}), 'Warren': Counter({'pos': 100, 'neg': 88}), 'Kamala': Counter({'neg': 19, 'pos': 15})}\n",
      "{'Biden': Counter({'pos': 23, 'neg': 17}), 'Bernie': Counter({'pos': 4, 'neg': 1}), 'Warren': Counter({'pos': 10, 'neg': 7}), 'Kamala': Counter({'neg': 3, 'pos': 2})}\n",
      "{'Biden': Counter({'pos': 23, 'neg': 17}), 'Bernie': Counter({'pos': 4, 'neg': 1}), 'Warren': Counter({'pos': 10, 'neg': 7}), 'Kamala': Counter({'neg': 3, 'pos': 2})}\n",
      "{'Biden': Counter({'neg': 1070, 'pos': 988}), 'Bernie': Counter({'neg': 253, 'pos': 201}), 'Warren': Counter({'neg': 423, 'pos': 303}), 'Kamala': Counter({'neg': 139, 'pos': 115})}\n",
      "{'Biden': Counter({'neg': 1070, 'pos': 988}), 'Bernie': Counter({'neg': 253, 'pos': 201}), 'Warren': Counter({'neg': 423, 'pos': 303}), 'Kamala': Counter({'neg': 139, 'pos': 115})}\n"
     ]
    }
   ],
   "source": [
    "## smae as above block but with no_duplicates=1\n",
    "all_publishers2 = {}\n",
    "pubnames = [x for x in sources.keys()]\n",
    "for i, publisher in enumerate(pubnames):\n",
    "   # pubs.append(get_sentiment_scores(names_list,publisher))\n",
    "    scores = get_sentiment_scores(names_list, publisher, 'tokenized_sents', 1)\n",
    "    print(scores)\n",
    "    all_publishers2[publisher] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to look into this tokenizer, it should tokenize paragraphs but i couldn't install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from estnltk import Tokenizer\n",
    "# tokenizer = Tokenizer()\n",
    "# document = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra code from previous project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "def name_entity_recognition(sentence):\n",
    "    '''\n",
    "    A function to retrieve name entities in a sentence.\n",
    "    :param sentence: the sentence to retrieve names from.\n",
    "    :return: a name entity list of the sentence.\n",
    "    '''\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    # retrieve person and organization's name from the sentence\n",
    "    name_entity = [x for x in doc.ents if x.label_ in ['PERSON']]\n",
    "    # convert all names to lowercase and remove 's and ’s in names\n",
    "    name_entity = [str(x).lower().replace(\"'s\", \"\") for x in name_entity]\n",
    "    name_entity = [x.replace(\"’s\", \"\") for x in name_entity]\n",
    "    # remove name words that are less than 3 letters to raise recognition accuracy\n",
    "    name_entity = [x for x in name_entity if len(x) >= 3]\n",
    "\n",
    "    return name_entity\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    \"\"\"A function that flattens a complex list\"\"\"\n",
    "    flat_list = []\n",
    "    for i in l:\n",
    "        for j in i:\n",
    "            flat_list.append(j)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "def nlist(book):\n",
    "    \"\"\"Returns a unique list of names from a sentence tokenized book\"\"\"\n",
    "    names = []\n",
    "    for i in book:\n",
    "        if name_entity_recognition(i) != []:\n",
    "            names.append(name_entity_recognition(i))\n",
    "    names = list(set(flatten(names)))\n",
    "    return names\n",
    "\n",
    "\n",
    "def top_names(name_list, novel, top_num=25):\n",
    "    '''\n",
    "    Returns name freq of a book for each name\n",
    "    '''\n",
    "\n",
    "    vect = CountVectorizer(vocabulary=name_list, stop_words='english')\n",
    "    name_frequency = vect.fit_transform([novel.lower()])\n",
    "    name_frequency = pd.DataFrame(\n",
    "        name_frequency.toarray(), columns=vect.get_feature_names())\n",
    "    name_frequency = name_frequency.T\n",
    "    name_frequency = name_frequency.sort_values(by=0, ascending=False)\n",
    "    name_frequency = name_frequency[0:top_num]\n",
    "    names = list(name_frequency.index)\n",
    "    name_frequency = list(name_frequency[0])\n",
    "\n",
    "    return name_frequency, names\n",
    "\n",
    "\n",
    "def name_freq_plot(df, title):\n",
    "    \"\"\"plot for name freq\"\"\"\n",
    "    sns.barplot(data=df,\n",
    "                y=df.names,\n",
    "                x=df.freq,\n",
    "                color='blue')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds a dictionary which stores all the names used in all articles from CNN via Spacy-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will build a dictionary which stores all the names used in all articles from CNN\n",
    "sources[\"CNN\"]['tokenized_sents'] = sources[\"CNN\"].apply(\n",
    "    lambda row: nltk.sent_tokenize(row['full_art']), axis=1)\n",
    "article_dict = {}\n",
    "for i, x in enumerate(sources[\"CNN\"]['tokenized_sents']):\n",
    "    article_dict[i] = nlist(x)\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "638.212px",
    "left": "1326.79px",
    "right": "20px",
    "top": "120px",
    "width": "248.767px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
