{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "from nltk import sent_tokenize\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from afinn import Afinn\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search\n",
    "import s1_analysis\n",
    "import collections\n",
    "s1 = s1_analysis.s1_analysis()\n",
    "s1.train_load(\"s1_model_2.sav\")\n",
    "pubs = []\n",
    "names_list = [\"Biden\", \"Bernie\", \"Warren\", \"Kamala\"]\n",
    "\n",
    "\n",
    "# read in data and sentence tokenize it\n",
    "df = pd.read_csv('vikas.csv')\n",
    "df['tokenized_sents'] = df.apply(\n",
    "    lambda row: nltk.sent_tokenize(row['full_art']), axis=1)\n",
    "\n",
    "\n",
    "# split df into dictionary with keys representing publisher and values representing df for those publishers\n",
    "sources = {}\n",
    "for source, df_source in df.groupby('source'):\n",
    "    sources[source] = df_source\n",
    "\n",
    "# grabs all sentences which mention candidate\n",
    "\n",
    "\n",
    "def get_candidate_mentions(candidate_name, publisher, tokenize_level):\n",
    "    mentions = []\n",
    "    substring = candidate_name\n",
    "    for article in sources[publisher][tokenize_level]:\n",
    "        for sentence in article:\n",
    "            if search(substring, sentence):\n",
    "                mentions.append(sentence)\n",
    "            else:\n",
    "                continue\n",
    "    return(mentions)\n",
    "\n",
    "\n",
    "def get_sentiment_scores(names_list, publisher, tokenize_level, no_duplicates=None):\n",
    "   # will store cand name and sentiment scores\n",
    "    cand_sent = {}\n",
    "\n",
    "    # build cand mention lists\n",
    "    joe_biden_mentions = get_candidate_mentions(\n",
    "        \"Biden|Joe Biden\", publisher, tokenize_level)\n",
    "    bernie_mentions = get_candidate_mentions(\n",
    "        \"Bernie|Sanders\", publisher, tokenize_level)\n",
    "    Warren_mentions = get_candidate_mentions(\n",
    "        \"Warren|Elizabeth Warren\", publisher, tokenize_level)\n",
    "    Kamala_mentions = get_candidate_mentions(\n",
    "        \"Kamala|Harris\", publisher, tokenize_level)\n",
    "\n",
    "    # build lists for sentiment scoring loop\n",
    "    cand_list = [joe_biden_mentions, bernie_mentions,\n",
    "                 Warren_mentions, Kamala_mentions]\n",
    "\n",
    "    # If we want to eliminate sentences with multiple candididate mentions, we call fucntion\n",
    "    # with no_duplicates=1 and this block executes\n",
    "\n",
    "    if no_duplicates == 1:\n",
    "\n",
    "        # shared_sent will grab any shared sentences\n",
    "        shared_sent = []\n",
    "        for x in joe_biden_mentions:\n",
    "            if search(\"Bernie|Sanders|Warren|Elizabeth Warren|Kamala|Harris\", x):\n",
    "                shared_sent.append(x)\n",
    "        for x in bernie_mentions:\n",
    "            if search(\"Joe|Joe Biden|Warren|Elizabeth Warren|Kamala|Harris\", x):\n",
    "                shared_sent.append(x)\n",
    "        for x in Warren_mentions:\n",
    "            if search(\"Joe|Joe Biden|Bernie|Sanders|Kamala|Harris\", x):\n",
    "                shared_sent.append(x)\n",
    "        for x in Kamala_mentions:\n",
    "            if search(\"Joe|Joe Biden|Bernie|Sanders|Warren|Elizabeth Warren\", x):\n",
    "                shared_sent.append(x)\n",
    "\n",
    "        # use shared sent list to remove shared sent from each candidates sentence list\n",
    "        for cand in cand_list:\n",
    "            for sent in cand:\n",
    "                if sent in shared_sent:\n",
    "                    cand.remove(sent)\n",
    "\n",
    "    # run sentiment score on each candidate and get count/store in returned dictionary\n",
    "    for i, candidate in enumerate(cand_list):\n",
    "        features = []\n",
    "        results = []\n",
    "        features = [s1.find_features(x) for x in candidate]\n",
    "        results = [s1.classifier.classify(x) for x in features]\n",
    "        counter = collections.Counter(results)\n",
    "        cand_sent[names_list[i]] = counter\n",
    "    ##print(cand_sent)\n",
    "    return(cand_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Biden': Counter({'pos': 6, 'neg': 6}), 'Bernie': Counter({'neg': 5}), 'Warren': Counter({'neg': 7, 'pos': 6}), 'Kamala': Counter({'neg': 2})}\n",
      "{'Biden': Counter({'neg': 1525, 'pos': 1473}), 'Bernie': Counter({'neg': 538, 'pos': 484}), 'Warren': Counter({'neg': 602, 'pos': 519}), 'Kamala': Counter({'pos': 216, 'neg': 205})}\n",
      "{'Biden': Counter({'neg': 962, 'pos': 883}), 'Bernie': Counter({'neg': 306, 'pos': 276}), 'Warren': Counter({'neg': 434, 'pos': 387}), 'Kamala': Counter({'neg': 121, 'pos': 100})}\n",
      "{'Biden': Counter({'pos': 2126, 'neg': 2044}), 'Bernie': Counter({'neg': 574, 'pos': 482}), 'Warren': Counter({'neg': 726, 'pos': 624}), 'Kamala': Counter({'neg': 244, 'pos': 193})}\n",
      "{'Biden': Counter({'neg': 24, 'pos': 13}), 'Bernie': Counter({'neg': 30, 'pos': 20}), 'Warren': Counter({'neg': 41, 'pos': 20}), 'Kamala': Counter({'pos': 5, 'neg': 4})}\n",
      "{'Biden': Counter({'pos': 119, 'neg': 113}), 'Bernie': Counter({'neg': 38, 'pos': 32}), 'Warren': Counter({'neg': 40, 'pos': 32}), 'Kamala': Counter({'pos': 14, 'neg': 8})}\n",
      "{'Biden': Counter({'neg': 327, 'pos': 319}), 'Bernie': Counter({'neg': 184, 'pos': 182}), 'Warren': Counter({'pos': 227, 'neg': 222}), 'Kamala': Counter({'neg': 70, 'pos': 57})}\n",
      "{'Biden': Counter({'pos': 286, 'neg': 217}), 'Bernie': Counter({'neg': 166, 'pos': 141}), 'Warren': Counter({'neg': 198, 'pos': 195}), 'Kamala': Counter({'neg': 34, 'pos': 33})}\n",
      "{'Biden': Counter({'pos': 55, 'neg': 50}), 'Bernie': Counter({'pos': 19, 'neg': 6}), 'Warren': Counter({'pos': 36, 'neg': 34}), 'Kamala': Counter({'neg': 7, 'pos': 4})}\n",
      "{'Biden': Counter({'neg': 1393, 'pos': 1326}), 'Bernie': Counter({'neg': 673, 'pos': 549}), 'Warren': Counter({'neg': 846, 'pos': 668}), 'Kamala': Counter({'neg': 308, 'pos': 227})}\n"
     ]
    }
   ],
   "source": [
    "# will return a dictionary that as a key gives candidate name and as a value\n",
    "# gives another dictionary with candidates name as key and sentiment scores as values\n",
    "all_publishers = {}\n",
    "pubnames = [x for x in sources.keys()]\n",
    "for i, publisher in enumerate(pubnames):\n",
    "   # pubs.append(get_sentiment_scores(names_list,publisher))\n",
    "    scores = get_sentiment_scores(names_list, publisher, 'tokenized_sents')\n",
    "    print(scores)\n",
    "    all_publishers[publisher] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bloomberg': {'Biden': Counter({'pos': 6, 'neg': 6}),\n",
       "  'Bernie': Counter({'neg': 5}),\n",
       "  'Warren': Counter({'pos': 6, 'neg': 7}),\n",
       "  'Kamala': Counter({'neg': 2})},\n",
       " 'Breitbart News': {'Biden': Counter({'neg': 1525, 'pos': 1473}),\n",
       "  'Bernie': Counter({'neg': 538, 'pos': 484}),\n",
       "  'Warren': Counter({'neg': 602, 'pos': 519}),\n",
       "  'Kamala': Counter({'pos': 216, 'neg': 205})},\n",
       " 'CNN': {'Biden': Counter({'pos': 883, 'neg': 962}),\n",
       "  'Bernie': Counter({'neg': 306, 'pos': 276}),\n",
       "  'Warren': Counter({'pos': 387, 'neg': 434}),\n",
       "  'Kamala': Counter({'neg': 121, 'pos': 100})},\n",
       " 'Fox News': {'Biden': Counter({'neg': 2044, 'pos': 2126}),\n",
       "  'Bernie': Counter({'pos': 482, 'neg': 574}),\n",
       "  'Warren': Counter({'pos': 624, 'neg': 726}),\n",
       "  'Kamala': Counter({'neg': 244, 'pos': 193})},\n",
       " 'Google News': {'Biden': Counter({'pos': 13, 'neg': 24}),\n",
       "  'Bernie': Counter({'neg': 30, 'pos': 20}),\n",
       "  'Warren': Counter({'neg': 41, 'pos': 20}),\n",
       "  'Kamala': Counter({'neg': 4, 'pos': 5})},\n",
       " 'MSNBC': {'Biden': Counter({'pos': 119, 'neg': 113}),\n",
       "  'Bernie': Counter({'pos': 32, 'neg': 38}),\n",
       "  'Warren': Counter({'neg': 40, 'pos': 32}),\n",
       "  'Kamala': Counter({'pos': 14, 'neg': 8})},\n",
       " 'New York Magazine': {'Biden': Counter({'neg': 327, 'pos': 319}),\n",
       "  'Bernie': Counter({'neg': 184, 'pos': 182}),\n",
       "  'Warren': Counter({'neg': 222, 'pos': 227}),\n",
       "  'Kamala': Counter({'neg': 70, 'pos': 57})},\n",
       " 'The New York Times': {'Biden': Counter({'pos': 286, 'neg': 217}),\n",
       "  'Bernie': Counter({'neg': 166, 'pos': 141}),\n",
       "  'Warren': Counter({'pos': 195, 'neg': 198}),\n",
       "  'Kamala': Counter({'neg': 34, 'pos': 33})},\n",
       " 'The Wall Street Journal': {'Biden': Counter({'pos': 55, 'neg': 50}),\n",
       "  'Bernie': Counter({'pos': 19, 'neg': 6}),\n",
       "  'Warren': Counter({'neg': 34, 'pos': 36}),\n",
       "  'Kamala': Counter({'neg': 7, 'pos': 4})},\n",
       " 'The Washington Post': {'Biden': Counter({'neg': 1393, 'pos': 1326}),\n",
       "  'Bernie': Counter({'pos': 549, 'neg': 673}),\n",
       "  'Warren': Counter({'neg': 846, 'pos': 668}),\n",
       "  'Kamala': Counter({'neg': 308, 'pos': 227})}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Biden': Counter({'neg': 4, 'pos': 3}), 'Bernie': Counter({'neg': 2}), 'Warren': Counter({'neg': 4, 'pos': 4}), 'Kamala': Counter({'neg': 1})}\n",
      "{'Biden': Counter({'neg': 1428, 'pos': 1359}), 'Bernie': Counter({'neg': 440, 'pos': 388}), 'Warren': Counter({'neg': 494, 'pos': 383}), 'Kamala': Counter({'pos': 179, 'neg': 170})}\n",
      "{'Biden': Counter({'neg': 896, 'pos': 799}), 'Bernie': Counter({'neg': 243, 'pos': 207}), 'Warren': Counter({'neg': 344, 'pos': 284}), 'Kamala': Counter({'neg': 98, 'pos': 80})}\n",
      "{'Biden': Counter({'neg': 1958, 'pos': 1943}), 'Bernie': Counter({'neg': 458, 'pos': 349}), 'Warren': Counter({'neg': 578, 'pos': 430}), 'Kamala': Counter({'neg': 219, 'pos': 159})}\n",
      "{'Biden': Counter({'neg': 16, 'pos': 12}), 'Bernie': Counter({'neg': 20, 'pos': 15}), 'Warren': Counter({'neg': 27, 'pos': 15}), 'Kamala': Counter({'pos': 5, 'neg': 1})}\n",
      "{'Biden': Counter({'pos': 106, 'neg': 101}), 'Bernie': Counter({'neg': 28, 'pos': 24}), 'Warren': Counter({'neg': 26, 'pos': 19}), 'Kamala': Counter({'pos': 12, 'neg': 5})}\n",
      "{'Biden': Counter({'neg': 285, 'pos': 253}), 'Bernie': Counter({'neg': 142, 'pos': 122}), 'Warren': Counter({'neg': 165, 'pos': 151}), 'Kamala': Counter({'neg': 54, 'pos': 36})}\n",
      "{'Biden': Counter({'pos': 238, 'neg': 190}), 'Bernie': Counter({'neg': 134, 'pos': 106}), 'Warren': Counter({'neg': 157, 'pos': 144}), 'Kamala': Counter({'neg': 29, 'pos': 24})}\n",
      "{'Biden': Counter({'neg': 48, 'pos': 47}), 'Bernie': Counter({'pos': 10, 'neg': 5}), 'Warren': Counter({'pos': 30, 'neg': 29}), 'Kamala': Counter({'neg': 7, 'pos': 2})}\n",
      "{'Biden': Counter({'neg': 1281, 'pos': 1193}), 'Bernie': Counter({'neg': 533, 'pos': 406}), 'Warren': Counter({'neg': 685, 'pos': 499}), 'Kamala': Counter({'neg': 261, 'pos': 179})}\n"
     ]
    }
   ],
   "source": [
    "## smae as above block but with no_duplicates=1\n",
    "all_publishers2 = {}\n",
    "pubnames = [x for x in sources.keys()]\n",
    "for i, publisher in enumerate(pubnames):\n",
    "   # pubs.append(get_sentiment_scores(names_list,publisher))\n",
    "    scores = get_sentiment_scores(names_list, publisher, 'tokenized_sents', 1)\n",
    "    print(scores)\n",
    "    all_publishers2[publisher] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to look into this tokenizer, it should tokenize paragraphs but i couldn't install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from estnltk import Tokenizer\n",
    "# tokenizer = Tokenizer()\n",
    "# document = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra code from previous project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "def name_entity_recognition(sentence):\n",
    "    '''\n",
    "    A function to retrieve name entities in a sentence.\n",
    "    :param sentence: the sentence to retrieve names from.\n",
    "    :return: a name entity list of the sentence.\n",
    "    '''\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    # retrieve person and organization's name from the sentence\n",
    "    name_entity = [x for x in doc.ents if x.label_ in ['PERSON']]\n",
    "    # convert all names to lowercase and remove 's and ’s in names\n",
    "    name_entity = [str(x).lower().replace(\"'s\", \"\") for x in name_entity]\n",
    "    name_entity = [x.replace(\"’s\", \"\") for x in name_entity]\n",
    "    # remove name words that are less than 3 letters to raise recognition accuracy\n",
    "    name_entity = [x for x in name_entity if len(x) >= 3]\n",
    "\n",
    "    return name_entity\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    \"\"\"A function that flattens a complex list\"\"\"\n",
    "    flat_list = []\n",
    "    for i in l:\n",
    "        for j in i:\n",
    "            flat_list.append(j)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "def nlist(book):\n",
    "    \"\"\"Returns a unique list of names from a sentence tokenized book\"\"\"\n",
    "    names = []\n",
    "    for i in book:\n",
    "        if name_entity_recognition(i) != []:\n",
    "            names.append(name_entity_recognition(i))\n",
    "    names = list(set(flatten(names)))\n",
    "    return names\n",
    "\n",
    "\n",
    "def top_names(name_list, novel, top_num=25):\n",
    "    '''\n",
    "    Returns name freq of a book for each name\n",
    "    '''\n",
    "\n",
    "    vect = CountVectorizer(vocabulary=name_list, stop_words='english')\n",
    "    name_frequency = vect.fit_transform([novel.lower()])\n",
    "    name_frequency = pd.DataFrame(\n",
    "        name_frequency.toarray(), columns=vect.get_feature_names())\n",
    "    name_frequency = name_frequency.T\n",
    "    name_frequency = name_frequency.sort_values(by=0, ascending=False)\n",
    "    name_frequency = name_frequency[0:top_num]\n",
    "    names = list(name_frequency.index)\n",
    "    name_frequency = list(name_frequency[0])\n",
    "\n",
    "    return name_frequency, names\n",
    "\n",
    "\n",
    "def name_freq_plot(df, title):\n",
    "    \"\"\"plot for name freq\"\"\"\n",
    "    sns.barplot(data=df,\n",
    "                y=df.names,\n",
    "                x=df.freq,\n",
    "                color='blue')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds a dictionary which stores all the names used in all articles from CNN via Spacy-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will build a dictionary which stores all the names used in all articles from CNN\n",
    "sources[\"CNN\"]['tokenized_sents'] = sources[\"CNN\"].apply(\n",
    "    lambda row: nltk.sent_tokenize(row['full_art']), axis=1)\n",
    "article_dict = {}\n",
    "for i, x in enumerate(sources[\"CNN\"]['tokenized_sents']):\n",
    "    article_dict[i] = nlist(x)\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "638.212px",
    "left": "1326.79px",
    "right": "20px",
    "top": "120px",
    "width": "248.767px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
