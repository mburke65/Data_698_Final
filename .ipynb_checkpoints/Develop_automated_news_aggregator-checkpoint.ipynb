{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project to automate api access \n",
    "+ I added my api key as config.py file  PLease get your api key https://newsapi.org/docs/get-started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import config\n",
    "import math\n",
    "from newsapi import NewsApiClient\n",
    "import newspaper\n",
    "import requests\n",
    "from newspaper import fulltext\n",
    "\n",
    "# Hit Api with credentials\n",
    "newsapi = NewsApiClient(api_key=config.api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all sources\n",
    "+ read through available sources list and make df storing domain and source name\n",
    "+ I hit a ton of sources below, we can clearly narrow it down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        domains                      sources\n",
      "0                        https://abcnews.go.com                     abc-news\n",
      "1                    http://www.abc.net.au/news                  abc-news-au\n",
      "2                    https://www.aftenposten.no                  aftenposten\n",
      "3                      http://www.aljazeera.com           al-jazeera-english\n",
      "4                            http://www.ansa.it                         ansa\n",
      "5                         http://www.argaam.com                       argaam\n",
      "6                        http://arstechnica.com                 ars-technica\n",
      "7                        https://arynews.tv/ud/                     ary-news\n",
      "8                           https://apnews.com/             associated-press\n",
      "9                            http://www.afr.com  australian-financial-review\n",
      "10                        https://www.axios.com                        axios\n",
      "11                    http://www.bbc.co.uk/news                     bbc-news\n",
      "12                   http://www.bbc.co.uk/sport                    bbc-sport\n",
      "13                           http://www.bild.de                         bild\n",
      "14                   http://br.blastingnews.com             blasting-news-br\n",
      "15                http://www.bleacherreport.com              bleacher-report\n",
      "16                     http://www.bloomberg.com                    bloomberg\n",
      "17                     http://www.breitbart.com               breitbart-news\n",
      "18               http://www.businessinsider.com             business-insider\n",
      "19                http://uk.businessinsider.com          business-insider-uk\n",
      "20                     https://www.buzzfeed.com                     buzzfeed\n",
      "21                       http://www.cbc.ca/news                     cbc-news\n",
      "22                       http://www.cbsnews.com                     cbs-news\n",
      "23                          http://www.cnbc.com                         cnbc\n",
      "24                            http://us.cnn.com                          cnn\n",
      "25                   http://cnnespanol.cnn.com/                       cnn-es\n",
      "26                          https://www.ccn.com            crypto-coins-news\n",
      "27   http://www.dailymail.co.uk/home/index.html                   daily-mail\n",
      "28                   http://www.tagesspiegel.de             der-tagesspiegel\n",
      "29                     http://www.zeit.de/index                     die-zeit\n",
      "..                                          ...                          ...\n",
      "104                               http://t3n.de                          t3n\n",
      "105                        http://talksport.com                    talksport\n",
      "106                      https://techcrunch.com                   techcrunch\n",
      "107                       https://techcrunch.cn                techcrunch-cn\n",
      "108                    http://www.techradar.com                    techradar\n",
      "109     http://www.theamericanconservative.com/    the-american-conservative\n",
      "110             https://www.theglobeandmail.com           the-globe-and-mail\n",
      "111                          http://thehill.com                     the-hill\n",
      "112                     http://www.thehindu.com                    the-hindu\n",
      "113               http://www.huffingtonpost.com          the-huffington-post\n",
      "114                  https://www.irishtimes.com              the-irish-times\n",
      "115                      https://www.jpost.com/           the-jerusalem-post\n",
      "116                  http://www.theladbible.com                the-lad-bible\n",
      "117                      http://www.nytimes.com           the-new-york-times\n",
      "118                       http://thenextweb.com                 the-next-web\n",
      "119                http://www.thesportbible.com              the-sport-bible\n",
      "120                  http://www.telegraph.co.uk                the-telegraph\n",
      "121          http://timesofindia.indiatimes.com           the-times-of-india\n",
      "122                     http://www.theverge.com                    the-verge\n",
      "123                          http://www.wsj.com      the-wall-street-journal\n",
      "124              https://www.washingtonpost.com          the-washington-post\n",
      "125            https://www.washingtontimes.com/         the-washington-times\n",
      "126                             http://time.com                         time\n",
      "127                http://www.usatoday.com/news                    usa-today\n",
      "128                       https://news.vice.com                    vice-news\n",
      "129                       https://www.wired.com                        wired\n",
      "130                        https://www.wired.de                     wired-de\n",
      "131                          http://www.wiwo.de            wirtschafts-woche\n",
      "132                       http://xinhuanet.com/                   xinhua-net\n",
      "133                       http://www.ynet.co.il                         ynet\n",
      "\n",
      "[134 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sources = newsapi.get_sources()\n",
    "new_orgs = sources[\"sources\"]\n",
    "my_sources = {}\n",
    "for i, x in enumerate(new_orgs):\n",
    "    my_sources[i] = (x['id'])\n",
    "domains = sources[\"sources\"]\n",
    "my_domains = {}\n",
    "for i, x in enumerate(domains):\n",
    "    my_domains[i] = (x['url'])\n",
    "sources = pd.Series(my_sources).to_frame(\"sources\")\n",
    "domains = pd.Series(my_domains).to_frame(\"domains\")\n",
    "query_keys_df = domains.join(sources)\n",
    "print(query_keys_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing data sources\n",
    "+ Lets attempt to grab some sources from different geographic locations as well as different idological perspectives\n",
    "\n",
    "+ categorizing news sources\n",
    "    + Traditional TV MSM\n",
    "        +  http://us.cnn.com   \n",
    "        +  http://www.cnbc.com \n",
    "        +  http://www.foxnews.com  \n",
    "        +  http://www.msnbc.com  \n",
    "        +  https://abcnews.go.com  \n",
    "        +  http://www.nbcnews.com  \n",
    "    + Traditional publications \n",
    "        +  http://www.nytimes.com  \n",
    "        +  https://www.washingtonpost.com \n",
    "        \n",
    "    + Internet Sources\n",
    "        +  http://www.huffingtonpost.com \n",
    "        +  https://www.politico.com\n",
    "        +  http://www.breitbart.com \n",
    "        +  https://news.google.com \n",
    "        +  https://www.buzzfeed.com \n",
    "        +  https://news.vice.com  \n",
    "    + Financial publications\n",
    "        +  http://www.economist.com\n",
    "        +  http://www.bloomberg.com \n",
    "        +  http://www.businessinsider.com \n",
    "        +  http://www.wsj.com\n",
    "        +  http://fortune.com  \n",
    "        \n",
    "    + News aggregators\n",
    "        +  https://apnews.com/ \n",
    "        +  http://www.reuters.com \n",
    "    + foreign reporting\n",
    "         + http://www.aljazeera.com  \n",
    "         + http://www.bbc.co.uk/news   \n",
    "         + https://www.jpost.com/  \n",
    "         + http://timesofindia.indiatimes.com \n",
    "         + https://russian.rt.com \n",
    "         + https://www.theguardian.com/uk \n",
    "         + http://www.independent.co.uk  \n",
    "         + http://www.telegraph.co.uk  \n",
    "\n",
    "\n",
    " \n",
    "### Build Query String "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc-news,al-jazeera-english,associated-press,bbc-news,bloomberg,breitbart-news,business-insider,buzzfeed,cbs-news,cnbc,cnn,four-four-two,fox-sports,google-news-ar,infobae,nbc-news,news24,polygon,rt,rte,the-hill,the-irish-times,the-new-york-times,the-sport-bible,the-times-of-india,the-washington-post,usa-today,vice-news,xinhua-net'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Literally picking data sources from df i printed above \n",
    "a= query_keys_df.iloc[[0,3,8,11,16,17,18,20,22,23,24,39,41,44,62,82,83,93,98,99,111,114,117,119,121,124,127,128,132],[1]]\n",
    "list_sources =a[\"sources\"].tolist()\n",
    "\n",
    "## build out string for query request \n",
    "myString = \",\".join(list_sources)\n",
    "myString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets begin process of automating query calls\n",
    "+ first lets build function to clean query returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(query):\n",
    "    for x in query['articles']:\n",
    "        try:\n",
    "            x[\"source\"] = x[\"source\"][\"name\"]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            x['publishedAt'] = str.split(x['publishedAt'], \"T\")[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del x['urlToImage']\n",
    "        except KeyError:\n",
    "            pass\n",
    "    my_df = pd.DataFrame(query[\"articles\"])\n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to hit the api\n",
    "+ Originally I had a loop here.  instead I figured I would just build a function that takes start data, end data, query term(candidate)\n",
    "    + The original code kept giving me a query limit reached result, so I decided to change up strategy and search one day at a time\n",
    "    + After we hit the papers at the start for past 30 days, we will only need 1 day at a time going forward.\n",
    "    + I built in some print statements for error handeling, which you will see below in the block after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "candidates_list=[]\n",
    "total_count=[]\n",
    "\n",
    "# Make first call\n",
    "def hit_api(start,end,q,myString):\n",
    "    \n",
    "    ## catch bug with formatted strings for dates\n",
    "    if end < 10:\n",
    "        start_str = \"0\"+ str(start)\n",
    "        end_str = \"0\"+ str(end)\n",
    "    elif end==10:\n",
    "        start_str = \"0\"+ str(start)\n",
    "        end_str = str(end)\n",
    "    else :\n",
    "        start_str = str(start)\n",
    "        end_str = str(end)\n",
    "      \n",
    "    ## API query\n",
    "    all_articles = newsapi.get_everything(q=q,\n",
    "                                          sources=myString,\n",
    "                                          domains='https://apnews.com/,http://www.nytimes.com',\n",
    "                                          language='en',\n",
    "                                          from_param='2019-09-{}'.format(start_str),\n",
    "                                          to='2019-09-{}'.format(end_str),\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=100,\n",
    "                                          page=1)\n",
    "    \n",
    "    ## get count\n",
    "    total_pages = math.ceil(all_articles[\"totalResults\"]/100)\n",
    "    print(\"query will return: \"+ str(all_articles[\"totalResults\"]))\n",
    "    \n",
    "    ## store count to check versus dimension of df later\n",
    "    total_count.append(all_articles[\"totalResults\"])\n",
    "    \n",
    "    ## Clen query \n",
    "    all_articles = clean_query(all_articles)\n",
    "    \n",
    "    ## append to list\n",
    "    candidates_list.append(all_articles)\n",
    "    return(candidates_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built out a loop \n",
    "+ Simple, look at first day of September to last day incrementing start and end by 1 each time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query will return: 48\n",
      "query will return: 76\n",
      "query will return: 94\n",
      "query will return: 84\n",
      "query will return: 56\n",
      "query will return: 47\n",
      "query will return: 60\n",
      "query will return: 81\n",
      "query will return: 116\n",
      "query will return: 151\n",
      "query will return: 268\n",
      "query will return: 213\n",
      "query will return: 64\n",
      "query will return: 83\n",
      "query will return: 106\n",
      "query will return: 127\n",
      "query will return: 152\n",
      "query will return: 136\n",
      "query will return: 86\n",
      "query will return: 68\n",
      "query will return: 76\n",
      "query will return: 103\n",
      "query will return: 95\n",
      "query will return: 65\n",
      "query will return: 57\n",
      "query will return: 36\n",
      "query will return: 29\n",
      "query will return: 59\n",
      "query will return: 131\n"
     ]
    }
   ],
   "source": [
    "for x in range(2,31):\n",
    "    df=hit_api(x,x+1,'Bernie|Sanders',myString)\n",
    "    \n",
    "## collapse list on itself to build big df\n",
    "Bernie_df = pd.concat(df)\n",
    "Bernie_df= Bernie_df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sanity checks\n",
    "\n",
    "### Issues\n",
    "+ The counts don't match up between expected and actual because the query results are limited to first 100 hits\n",
    "    + Won't be an issue with less publications, but we need to be careful splitting candidate calls up.\n",
    "+ Not all links are unique\n",
    "    + This is expected given internet sources reposting articles from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we should expect: 2767 articles\n",
      "we have: 2264 articles\n",
      "503 results came into query that exceeded 100 hits in a day.  Thus we lose any hit over 100 in a day\n",
      "we have 1234 unique links\n"
     ]
    }
   ],
   "source": [
    "##validate that df shape[0] is equal to expected query count\n",
    "print(\"we should expect: {} articles\".format(sum(total_count)))\n",
    "print(\"we have: {} articles\".format(Bernie_df.shape[0]))\n",
    "\n",
    "\n",
    "## test 100 results theory\n",
    "over_100 = sum([x-100 for x in total_count if x>100 ])\n",
    "print(\"{} results came into query that exceeded 100 hits in a day.  Thus we lose any hit over 100 in a day\".format(over_100))\n",
    "\n",
    "\n",
    "##unique links\n",
    "unique_array = Bernie_df.url.unique()\n",
    "print(\"we have {} unique links\".format(unique_array.shape[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicateRowsDF = Bernie_df[Bernie_df.duplicated(\"url\")]\n",
    "duplicateRowsDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>description</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Welcome to Majority.FM 's AM QUICKIE! Brought ...</td>\n",
       "      <td>2019-09-24</td>\n",
       "      <td>Google News</td>\n",
       "      <td>AM QUICKIE: September 24th, 2019 w/ Lucie Stei...</td>\n",
       "      <td>http://feedproxy.google.com/~r/MajorityReport/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Welcome to Majority.FM 's AM QUICKIE! Brought ...</td>\n",
       "      <td>2019-09-11</td>\n",
       "      <td>Google News</td>\n",
       "      <td>AM QUICKIE: September 11th, 2019 w/ Lucie Stei...</td>\n",
       "      <td>http://feedproxy.google.com/~r/MajorityReport/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Welcome to Majority.FM 's AM QUICKIE! Brought ...</td>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>Google News</td>\n",
       "      <td>AM QUICKIE: September 18th, 2019 w/ Lucie Stei...</td>\n",
       "      <td>http://feedproxy.google.com/~r/MajorityReport/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Welcome to Majority.FM 's AM QUICKIE! Brought ...</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>Google News</td>\n",
       "      <td>AM QUICKIE: September 20th, 2019 w/ Sam Seder ...</td>\n",
       "      <td>http://feedproxy.google.com/~r/MajorityReport/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Welcome to Majority.FM 's AM QUICKIE! Brought ...</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>Google News</td>\n",
       "      <td>AM QUICKIE: September 20th, 2019 w/ Sam Seder ...</td>\n",
       "      <td>http://feedproxy.google.com/~r/MajorityReport/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Welcome to Majority.FM 's AM QUICKIE! Brought ...</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>Google News</td>\n",
       "      <td>AM QUICKIE: September 17th, 2019 w/ Lucie Stei...</td>\n",
       "      <td>http://feedproxy.google.com/~r/MajorityReport/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Welcome to Majority.FM 's AM QUICKIE! Brought ...</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>Google News</td>\n",
       "      <td>AM QUICKIE: September 17th, 2019 w/ Lucie Stei...</td>\n",
       "      <td>http://feedproxy.google.com/~r/MajorityReport/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>Tom Woods, Tom Woods</td>\n",
       "      <td>None</td>\n",
       "      <td>Bernie Sanders is proposing a nationwide progr...</td>\n",
       "      <td>2019-09-24</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Ep. 1498 Against Bernie's National Rent Control</td>\n",
       "      <td>http://feedproxy.google.com/~r/TheTomWoodsShow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>Tom Woods, Tom Woods</td>\n",
       "      <td>None</td>\n",
       "      <td>Bernie Sanders is proposing a nationwide progr...</td>\n",
       "      <td>2019-09-24</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Ep. 1498 Against Bernie's National Rent Control</td>\n",
       "      <td>http://feedproxy.google.com/~r/TheTomWoodsShow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>Tom Woods, Tom Woods</td>\n",
       "      <td>None</td>\n",
       "      <td>Bernie Sanders is proposing a nationwide progr...</td>\n",
       "      <td>2019-09-24</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Ep. 1498 Against Bernie's National Rent Control</td>\n",
       "      <td>http://feedproxy.google.com/~r/TheTomWoodsShow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>Carmen Reinicke</td>\n",
       "      <td>None</td>\n",
       "      <td>The US Chamber of Commerce's Center for Capita...</td>\n",
       "      <td>2019-09-21</td>\n",
       "      <td>Google News</td>\n",
       "      <td>The US Chamber of Commerce released a report s...</td>\n",
       "      <td>http://feedproxy.google.com/~r/businessinsider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>Carmen Reinicke</td>\n",
       "      <td>None</td>\n",
       "      <td>The US Chamber of Commerce's Center for Capita...</td>\n",
       "      <td>2019-09-21</td>\n",
       "      <td>Google News</td>\n",
       "      <td>The US Chamber of Commerce released a report s...</td>\n",
       "      <td>http://feedproxy.google.com/~r/businessinsider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>Carmen Reinicke</td>\n",
       "      <td>None</td>\n",
       "      <td>Senator Bernie Sanders on Monday announced a n...</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Bernie Sanders unveils 'inequality tax' target...</td>\n",
       "      <td>http://feedproxy.google.com/~r/businessinsider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>Carmen Reinicke</td>\n",
       "      <td>None</td>\n",
       "      <td>Senator Bernie Sanders on Monday announced a n...</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Bernie Sanders unveils 'inequality tax' target...</td>\n",
       "      <td>http://feedproxy.google.com/~r/businessinsider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>feedback@businessinsider.com (Carmen Reinicke)...</td>\n",
       "      <td>None</td>\n",
       "      <td>Getty Images / Scott Eisen Senator Bernie Sand...</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Bernie Sanders unveils 'inequality tax' target...</td>\n",
       "      <td>http://feedproxy.google.com/~r/businessinsider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>feedback@businessinsider.com (Carmen Reinicke)...</td>\n",
       "      <td>None</td>\n",
       "      <td>Getty Images / Scott Eisen Senator Bernie Sand...</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Bernie Sanders unveils 'inequality tax' target...</td>\n",
       "      <td>http://feedproxy.google.com/~r/businessinsider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2178</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Bernie Sanders raises more than $25 million in...</td>\n",
       "      <td>http://feedproxy.google.com/~r/euronews/en/hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>We’ve gone from drought to flood. People who c...</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Does Climate Make Good Political TV?</td>\n",
       "      <td>http://feedproxy.google.com/~r/greentechmedia/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>We’ve gone from drought to flood. People who c...</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>Google News</td>\n",
       "      <td>Does Climate Make Good Political TV?</td>\n",
       "      <td>http://feedproxy.google.com/~r/greentechmedia/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 author content  \\\n",
       "1792                                               None    None   \n",
       "628                                                None    None   \n",
       "1280                                               None    None   \n",
       "1543                                               None    None   \n",
       "1470                                               None    None   \n",
       "1169                                               None    None   \n",
       "1288                                               None    None   \n",
       "1675                               Tom Woods, Tom Woods    None   \n",
       "1881                               Tom Woods, Tom Woods    None   \n",
       "1775                               Tom Woods, Tom Woods    None   \n",
       "1609                                    Carmen Reinicke    None   \n",
       "1536                                    Carmen Reinicke    None   \n",
       "2219                                    Carmen Reinicke    None   \n",
       "2134                                    Carmen Reinicke    None   \n",
       "2167  feedback@businessinsider.com (Carmen Reinicke)...    None   \n",
       "2105  feedback@businessinsider.com (Carmen Reinicke)...    None   \n",
       "2178                                               None    None   \n",
       "1242                                               None    None   \n",
       "1128                                               None    None   \n",
       "\n",
       "                                            description publishedAt  \\\n",
       "1792  Welcome to Majority.FM 's AM QUICKIE! Brought ...  2019-09-24   \n",
       "628   Welcome to Majority.FM 's AM QUICKIE! Brought ...  2019-09-11   \n",
       "1280  Welcome to Majority.FM 's AM QUICKIE! Brought ...  2019-09-18   \n",
       "1543  Welcome to Majority.FM 's AM QUICKIE! Brought ...  2019-09-20   \n",
       "1470  Welcome to Majority.FM 's AM QUICKIE! Brought ...  2019-09-20   \n",
       "1169  Welcome to Majority.FM 's AM QUICKIE! Brought ...  2019-09-17   \n",
       "1288  Welcome to Majority.FM 's AM QUICKIE! Brought ...  2019-09-17   \n",
       "1675  Bernie Sanders is proposing a nationwide progr...  2019-09-24   \n",
       "1881  Bernie Sanders is proposing a nationwide progr...  2019-09-24   \n",
       "1775  Bernie Sanders is proposing a nationwide progr...  2019-09-24   \n",
       "1609  The US Chamber of Commerce's Center for Capita...  2019-09-21   \n",
       "1536  The US Chamber of Commerce's Center for Capita...  2019-09-21   \n",
       "2219  Senator Bernie Sanders on Monday announced a n...  2019-09-30   \n",
       "2134  Senator Bernie Sanders on Monday announced a n...  2019-09-30   \n",
       "2167  Getty Images / Scott Eisen Senator Bernie Sand...  2019-09-30   \n",
       "2105  Getty Images / Scott Eisen Senator Bernie Sand...  2019-09-30   \n",
       "2178                                                     2019-10-01   \n",
       "1242  We’ve gone from drought to flood. People who c...  2019-09-17   \n",
       "1128  We’ve gone from drought to flood. People who c...  2019-09-17   \n",
       "\n",
       "           source                                              title  \\\n",
       "1792  Google News  AM QUICKIE: September 24th, 2019 w/ Lucie Stei...   \n",
       "628   Google News  AM QUICKIE: September 11th, 2019 w/ Lucie Stei...   \n",
       "1280  Google News  AM QUICKIE: September 18th, 2019 w/ Lucie Stei...   \n",
       "1543  Google News  AM QUICKIE: September 20th, 2019 w/ Sam Seder ...   \n",
       "1470  Google News  AM QUICKIE: September 20th, 2019 w/ Sam Seder ...   \n",
       "1169  Google News  AM QUICKIE: September 17th, 2019 w/ Lucie Stei...   \n",
       "1288  Google News  AM QUICKIE: September 17th, 2019 w/ Lucie Stei...   \n",
       "1675  Google News    Ep. 1498 Against Bernie's National Rent Control   \n",
       "1881  Google News    Ep. 1498 Against Bernie's National Rent Control   \n",
       "1775  Google News    Ep. 1498 Against Bernie's National Rent Control   \n",
       "1609  Google News  The US Chamber of Commerce released a report s...   \n",
       "1536  Google News  The US Chamber of Commerce released a report s...   \n",
       "2219  Google News  Bernie Sanders unveils 'inequality tax' target...   \n",
       "2134  Google News  Bernie Sanders unveils 'inequality tax' target...   \n",
       "2167  Google News  Bernie Sanders unveils 'inequality tax' target...   \n",
       "2105  Google News  Bernie Sanders unveils 'inequality tax' target...   \n",
       "2178  Google News  Bernie Sanders raises more than $25 million in...   \n",
       "1242  Google News               Does Climate Make Good Political TV?   \n",
       "1128  Google News               Does Climate Make Good Political TV?   \n",
       "\n",
       "                                                    url  \n",
       "1792  http://feedproxy.google.com/~r/MajorityReport/...  \n",
       "628   http://feedproxy.google.com/~r/MajorityReport/...  \n",
       "1280  http://feedproxy.google.com/~r/MajorityReport/...  \n",
       "1543  http://feedproxy.google.com/~r/MajorityReport/...  \n",
       "1470  http://feedproxy.google.com/~r/MajorityReport/...  \n",
       "1169  http://feedproxy.google.com/~r/MajorityReport/...  \n",
       "1288  http://feedproxy.google.com/~r/MajorityReport/...  \n",
       "1675  http://feedproxy.google.com/~r/TheTomWoodsShow...  \n",
       "1881  http://feedproxy.google.com/~r/TheTomWoodsShow...  \n",
       "1775  http://feedproxy.google.com/~r/TheTomWoodsShow...  \n",
       "1609  http://feedproxy.google.com/~r/businessinsider...  \n",
       "1536  http://feedproxy.google.com/~r/businessinsider...  \n",
       "2219  http://feedproxy.google.com/~r/businessinsider...  \n",
       "2134  http://feedproxy.google.com/~r/businessinsider...  \n",
       "2167  http://feedproxy.google.com/~r/businessinsider...  \n",
       "2105  http://feedproxy.google.com/~r/businessinsider...  \n",
       "2178  http://feedproxy.google.com/~r/euronews/en/hom...  \n",
       "1242  http://feedproxy.google.com/~r/greentechmedia/...  \n",
       "1128  http://feedproxy.google.com/~r/greentechmedia/...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bernie_df.sort_values(by=['url'])[1:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hitting newspaper 3k Api with links\n",
    "+ We can deal with duplicates and streamlining the above later\n",
    "+ to demonstrate a working product I feed our url into newspaper 3k \n",
    "+ Id say it takes 5-10 seconds per article to fetch complete text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_full_text=[]\n",
    "for link in Bernie_df['url'][0:10]:\n",
    "    html = requests.get(link).text\n",
    "    text = fulltext(html)\n",
    "    list_full_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ed O\\'Keefe CBS News\\n\\nEd O\\'Keefe is a CBS News political correspondent based in Washington, D.C., covering the 2020 presidential campaign for all CBS News broadcasts and platforms.\\n\\nIn the early months of the campaign, he has interviewed several Democratic contenders, including Steve Bullock, Pete Buttigieg, Julian Castro, Kirsten Gillibrand, Kamala Harris, Amy Klobuchar, Elizabeth Warren and Bernie Sanders and Republican William Weld.\\n\\nSince joining CBS News in April 2018, O\\'Keefe has contributed to coverage of the 2018 midterm elections, the contentious confirmation hearing for Supreme Court Justice Brett Kavanaugh, the funerals of former president George H.W. Bush and former Arizona Senator John McCain, the record-long federal government shutdown and the blackface scandal and sexual misconduct allegations that rocked Virginia state government in early 2019.\\n\\nTrending News CBS News Internship Program\\n\\nBefore joining CBS News, O\\'Keefe spent nearly 13 years at The Washington Post covering congressional and presidential elections, Capitol Hill and federal agencies. He first joined CBS News as a contributor in 2017.\\n\\nDuring the 2008 campaign, , O\\'Keefe covered or contributed reporting on the presidential campaigns of Jeb Bush, Hillary Clinton, John McCain, Barack Obama, Mitt Romney, Marco Rubio and Donald Trump, as well as dozens of key House and Senate races. On Capitol Hill, he reported extensively on debates over gun control, immigration policy, health-care reform and federal spending; the \"fiscal cliff\" showdown of late 2012; two government shutdowns; the confirmation of Supreme Court Justice Neil Gorsuch; and dozens of contentious congressional hearings, including former FBI Director James Comey\\'s dramatic testimony last year.\\n\\nO\\'Keefe\\'s coverage also included reporting on federal agencies and a range of policy issues, from the repeal of the Pentagon\\'s \"don\\'t ask, don\\'t tell\" policy, to the decline of the U.S. Postal Service and cases of questionable government spending. He also briefly covered the war in Iraq.\\n\\nO\\'Keefe is a member of the National Association of Hispanic Journalists. He is a graduate of American University and was born and raised in Delmar, N.Y.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_full_text[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "+ we need to functionize streamline and clean up query calls.\n",
    "+ sorry my python is rusty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
