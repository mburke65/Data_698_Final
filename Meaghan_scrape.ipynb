{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project to automate api access \n",
    "+ I added my api key as config.py file  PLease get your api key https://newsapi.org/docs/get-started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import config\n",
    "import math\n",
    "from newsapi import NewsApiClient\n",
    "import newspaper\n",
    "import requests\n",
    "from newspaper import fulltext\n",
    "import time\n",
    "# Hit Api with credentials\n",
    "newsapi = NewsApiClient(api_key=config.api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all sources\n",
    "+ read through available sources list and make df storing domain and source name\n",
    "+ I hit a ton of sources below, we can clearly narrow it down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domains</th>\n",
       "      <th>sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://abcnews.go.com</td>\n",
       "      <td>abc-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.abc.net.au/news</td>\n",
       "      <td>abc-news-au</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.aftenposten.no</td>\n",
       "      <td>aftenposten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.aljazeera.com</td>\n",
       "      <td>al-jazeera-english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.ansa.it</td>\n",
       "      <td>ansa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.argaam.com</td>\n",
       "      <td>argaam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://arstechnica.com</td>\n",
       "      <td>ars-technica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://arynews.tv/ud/</td>\n",
       "      <td>ary-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://apnews.com/</td>\n",
       "      <td>associated-press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://www.afr.com</td>\n",
       "      <td>australian-financial-review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.axios.com</td>\n",
       "      <td>axios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://www.bbc.co.uk/news</td>\n",
       "      <td>bbc-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://www.bbc.co.uk/sport</td>\n",
       "      <td>bbc-sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://www.bild.de</td>\n",
       "      <td>bild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://br.blastingnews.com</td>\n",
       "      <td>blasting-news-br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>http://www.bleacherreport.com</td>\n",
       "      <td>bleacher-report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>http://www.bloomberg.com</td>\n",
       "      <td>bloomberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>http://www.breitbart.com</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>http://www.businessinsider.com</td>\n",
       "      <td>business-insider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>http://uk.businessinsider.com</td>\n",
       "      <td>business-insider-uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.buzzfeed.com</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>http://www.cbc.ca/news</td>\n",
       "      <td>cbc-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>http://www.cbsnews.com</td>\n",
       "      <td>cbs-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>http://www.cnbc.com</td>\n",
       "      <td>cnbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>http://us.cnn.com</td>\n",
       "      <td>cnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>http://cnnespanol.cnn.com/</td>\n",
       "      <td>cnn-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.ccn.com</td>\n",
       "      <td>crypto-coins-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>http://www.dailymail.co.uk/home/index.html</td>\n",
       "      <td>daily-mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>http://www.tagesspiegel.de</td>\n",
       "      <td>der-tagesspiegel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>http://www.zeit.de/index</td>\n",
       "      <td>die-zeit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>http://t3n.de</td>\n",
       "      <td>t3n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>http://talksport.com</td>\n",
       "      <td>talksport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://techcrunch.com</td>\n",
       "      <td>techcrunch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://techcrunch.cn</td>\n",
       "      <td>techcrunch-cn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>http://www.techradar.com</td>\n",
       "      <td>techradar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>http://www.theamericanconservative.com/</td>\n",
       "      <td>the-american-conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://www.theglobeandmail.com</td>\n",
       "      <td>the-globe-and-mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>http://thehill.com</td>\n",
       "      <td>the-hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>http://www.thehindu.com</td>\n",
       "      <td>the-hindu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>http://www.huffingtonpost.com</td>\n",
       "      <td>the-huffington-post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://www.irishtimes.com</td>\n",
       "      <td>the-irish-times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://www.jpost.com/</td>\n",
       "      <td>the-jerusalem-post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>http://www.theladbible.com</td>\n",
       "      <td>the-lad-bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>http://www.nytimes.com</td>\n",
       "      <td>the-new-york-times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>http://thenextweb.com</td>\n",
       "      <td>the-next-web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>http://www.thesportbible.com</td>\n",
       "      <td>the-sport-bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>http://www.telegraph.co.uk</td>\n",
       "      <td>the-telegraph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>http://timesofindia.indiatimes.com</td>\n",
       "      <td>the-times-of-india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>http://www.theverge.com</td>\n",
       "      <td>the-verge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>http://www.wsj.com</td>\n",
       "      <td>the-wall-street-journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>https://www.washingtonpost.com</td>\n",
       "      <td>the-washington-post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.washingtontimes.com/</td>\n",
       "      <td>the-washington-times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>http://time.com</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>http://www.usatoday.com/news</td>\n",
       "      <td>usa-today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>https://news.vice.com</td>\n",
       "      <td>vice-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>https://www.wired.com</td>\n",
       "      <td>wired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>https://www.wired.de</td>\n",
       "      <td>wired-de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>http://www.wiwo.de</td>\n",
       "      <td>wirtschafts-woche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>http://xinhuanet.com/</td>\n",
       "      <td>xinhua-net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>http://www.ynet.co.il</td>\n",
       "      <td>ynet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        domains                      sources\n",
       "0                        https://abcnews.go.com                     abc-news\n",
       "1                    http://www.abc.net.au/news                  abc-news-au\n",
       "2                    https://www.aftenposten.no                  aftenposten\n",
       "3                      http://www.aljazeera.com           al-jazeera-english\n",
       "4                            http://www.ansa.it                         ansa\n",
       "5                         http://www.argaam.com                       argaam\n",
       "6                        http://arstechnica.com                 ars-technica\n",
       "7                        https://arynews.tv/ud/                     ary-news\n",
       "8                           https://apnews.com/             associated-press\n",
       "9                            http://www.afr.com  australian-financial-review\n",
       "10                        https://www.axios.com                        axios\n",
       "11                    http://www.bbc.co.uk/news                     bbc-news\n",
       "12                   http://www.bbc.co.uk/sport                    bbc-sport\n",
       "13                           http://www.bild.de                         bild\n",
       "14                   http://br.blastingnews.com             blasting-news-br\n",
       "15                http://www.bleacherreport.com              bleacher-report\n",
       "16                     http://www.bloomberg.com                    bloomberg\n",
       "17                     http://www.breitbart.com               breitbart-news\n",
       "18               http://www.businessinsider.com             business-insider\n",
       "19                http://uk.businessinsider.com          business-insider-uk\n",
       "20                     https://www.buzzfeed.com                     buzzfeed\n",
       "21                       http://www.cbc.ca/news                     cbc-news\n",
       "22                       http://www.cbsnews.com                     cbs-news\n",
       "23                          http://www.cnbc.com                         cnbc\n",
       "24                            http://us.cnn.com                          cnn\n",
       "25                   http://cnnespanol.cnn.com/                       cnn-es\n",
       "26                          https://www.ccn.com            crypto-coins-news\n",
       "27   http://www.dailymail.co.uk/home/index.html                   daily-mail\n",
       "28                   http://www.tagesspiegel.de             der-tagesspiegel\n",
       "29                     http://www.zeit.de/index                     die-zeit\n",
       "..                                          ...                          ...\n",
       "104                               http://t3n.de                          t3n\n",
       "105                        http://talksport.com                    talksport\n",
       "106                      https://techcrunch.com                   techcrunch\n",
       "107                       https://techcrunch.cn                techcrunch-cn\n",
       "108                    http://www.techradar.com                    techradar\n",
       "109     http://www.theamericanconservative.com/    the-american-conservative\n",
       "110             https://www.theglobeandmail.com           the-globe-and-mail\n",
       "111                          http://thehill.com                     the-hill\n",
       "112                     http://www.thehindu.com                    the-hindu\n",
       "113               http://www.huffingtonpost.com          the-huffington-post\n",
       "114                  https://www.irishtimes.com              the-irish-times\n",
       "115                      https://www.jpost.com/           the-jerusalem-post\n",
       "116                  http://www.theladbible.com                the-lad-bible\n",
       "117                      http://www.nytimes.com           the-new-york-times\n",
       "118                       http://thenextweb.com                 the-next-web\n",
       "119                http://www.thesportbible.com              the-sport-bible\n",
       "120                  http://www.telegraph.co.uk                the-telegraph\n",
       "121          http://timesofindia.indiatimes.com           the-times-of-india\n",
       "122                     http://www.theverge.com                    the-verge\n",
       "123                          http://www.wsj.com      the-wall-street-journal\n",
       "124              https://www.washingtonpost.com          the-washington-post\n",
       "125            https://www.washingtontimes.com/         the-washington-times\n",
       "126                             http://time.com                         time\n",
       "127                http://www.usatoday.com/news                    usa-today\n",
       "128                       https://news.vice.com                    vice-news\n",
       "129                       https://www.wired.com                        wired\n",
       "130                        https://www.wired.de                     wired-de\n",
       "131                          http://www.wiwo.de            wirtschafts-woche\n",
       "132                       http://xinhuanet.com/                   xinhua-net\n",
       "133                       http://www.ynet.co.il                         ynet\n",
       "\n",
       "[134 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "sources = newsapi.get_sources()\n",
    "new_orgs = sources[\"sources\"]\n",
    "my_sources = {}\n",
    "for i, x in enumerate(new_orgs):\n",
    "    my_sources[i] = (x['id'])\n",
    "domains = sources[\"sources\"]\n",
    "my_domains = {}\n",
    "for i, x in enumerate(domains):\n",
    "    my_domains[i] = (x['url'])\n",
    "sources = pd.Series(my_sources).to_frame(\"sources\")\n",
    "domains = pd.Series(my_domains).to_frame(\"domains\")\n",
    "query_keys_df = domains.join(sources)\n",
    "(query_keys_df)\n",
    "#query_keys_df.to_csv('lists.csv')\n",
    "#3,16,17,24,40,77,124,123,18,117,81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing data sources\n",
    "+ Lets attempt to grab some sources from different geographic locations as well as different idological perspectives\n",
    "\n",
    "+ categorizing news sources\n",
    "    + Traditional TV MSM\n",
    "        +  http://us.cnn.com   \n",
    "        +  http://www.cnbc.com \n",
    "        +  http://www.foxnews.com  \n",
    "        +  http://www.msnbc.com  \n",
    "        +  https://abcnews.go.com  \n",
    "        +  http://www.nbcnews.com  \n",
    "    + Traditional publications \n",
    "        +  http://www.nytimes.com  \n",
    "        +  https://www.washingtonpost.com \n",
    "        \n",
    "    + Internet Sources\n",
    "        +  http://www.huffingtonpost.com \n",
    "        +  https://www.politico.com\n",
    "        +  http://www.breitbart.com \n",
    "        +  https://news.google.com \n",
    "        +  https://www.buzzfeed.com \n",
    "        +  https://news.vice.com  \n",
    "    + Financial publications\n",
    "        +  http://www.economist.com\n",
    "        +  http://www.bloomberg.com \n",
    "        +  http://www.businessinsider.com \n",
    "        +  http://www.wsj.com\n",
    "        +  http://fortune.com  \n",
    "        \n",
    "    + News aggregators\n",
    "        +  https://apnews.com/ \n",
    "        +  http://www.reuters.com \n",
    "    + foreign reporting\n",
    "         + http://www.aljazeera.com  \n",
    "         + http://www.bbc.co.uk/news   \n",
    "         + https://www.jpost.com/  \n",
    "         + http://timesofindia.indiatimes.com \n",
    "         + https://russian.rt.com \n",
    "         + https://www.theguardian.com/uk \n",
    "         + http://www.independent.co.uk  \n",
    "         + http://www.telegraph.co.uk  \n",
    "\n",
    "\n",
    " \n",
    "### Build Query String "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new-york-magazine,the-wall-street-journal,the-huffington-post,the-new-york-times,the-wall-street-journal,the-washington-post'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Literally picking data sources from df i printed above \n",
    "a= query_keys_df.iloc[[16,17,24,40,43,77],[1]]\n",
    "b= query_keys_df.iloc[[87,123,113,117,123,124],[1]]\n",
    "list_sources =a[\"sources\"].tolist()\n",
    "\n",
    "## build out string for query request \n",
    "myString = \",\".join(list_sources)\n",
    "\n",
    "list_sources2 = b[\"sources\"].tolist()\n",
    "myString2 = \",\".join(list_sources2)\n",
    "myString2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets begin process of automating query calls\n",
    "+ first lets build function to clean query returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(query):\n",
    "    for x in query['articles']:\n",
    "        try:\n",
    "            x[\"source\"] = x[\"source\"][\"name\"]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            x['publishedAt'] = str.split(x['publishedAt'], \"T\")[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del x['urlToImage']\n",
    "        except KeyError:\n",
    "            pass\n",
    "    my_df = pd.DataFrame(query[\"articles\"])\n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to hit the api\n",
    "+ Originally I had a loop here.  instead I figured I would just build a function that takes start data, end data, query term(candidate)\n",
    "    + The original code kept giving me a query limit reached result, so I decided to change up strategy and search one day at a time\n",
    "    + After we hit the papers at the start for past 30 days, we will only need 1 day at a time going forward.\n",
    "    + I built in some print statements for error handeling, which you will see below in the block after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "candidates_list=[]\n",
    "total_count=[]\n",
    "\n",
    "# Make first call\n",
    "def hit_api(start,end,q,myString2):\n",
    "    \n",
    "    ## catch bug with formatted strings for dates\n",
    "    if end < 10:\n",
    "        start_str = \"0\"+ str(start)\n",
    "        end_str = \"0\"+ str(end)\n",
    "    elif end==10:\n",
    "        start_str = \"0\"+ str(start)\n",
    "        end_str = str(end)\n",
    "    else :\n",
    "        start_str = str(start)\n",
    "        end_str = str(end)\n",
    "      \n",
    "    ## API query\n",
    "    all_articles = newsapi.get_everything(q=q,\n",
    "                                          sources=myString2,\n",
    "                                          language='en',\n",
    "                                          from_param='2019-09-{}'.format(start_str),\n",
    "                                          to='2019-09-{}'.format(end_str),\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=100,\n",
    "                                          page=1)\n",
    "    \n",
    "    ## get count\n",
    "    total_pages = math.ceil(all_articles[\"totalResults\"]/100)\n",
    "    print(\"query will return: \"+ str(all_articles[\"totalResults\"]))\n",
    "    \n",
    "    ## store count to check versus dimension of df later\n",
    "    total_count.append(all_articles[\"totalResults\"])\n",
    "    \n",
    "    ## Clen query \n",
    "    all_articles = clean_query(all_articles)\n",
    "    \n",
    "    ## append to list\n",
    "    candidates_list.append(all_articles)\n",
    "    return(candidates_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built out a loop (Important\n",
    "+ Simple, look at first day of September to last day incrementing start and end by 1 each time\n",
    "+ We need to expand on queires here, simply just change bernie or bernie sanders to Warren or Elizabeth warren, Biden OR Joe Biden etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query will return: 20\n",
      "query will return: 17\n",
      "query will return: 25\n",
      "query will return: 33\n",
      "query will return: 24\n",
      "query will return: 16\n",
      "query will return: 16\n",
      "query will return: 12\n",
      "query will return: 9\n",
      "query will return: 12\n",
      "query will return: 281\n"
     ]
    }
   ],
   "source": [
    "for x in range(20,31):\n",
    "    df=hit_api(x,x+1,'Bernie OR Bernie Sanders',myString2)\n",
    "    \n",
    "## collapse list on itself to build big df\n",
    "Bernie_df = pd.concat(df)\n",
    "Bernie_df= Bernie_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sanity checks\n",
    "\n",
    "### Issues\n",
    "+ The counts don't match up between expected and actual because the query results are limited to first 100 hits\n",
    "    + Won't be an issue with less publications, but we need to be careful splitting candidate calls up.\n",
    "+ Not all links are unique\n",
    "    + This is expected given internet sources reposting articles from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we should expect: 465 articles\n",
      "we have: 284 articles\n",
      "181 results came into query that exceeded 100 hits in a day.  Thus we lose any hit over 100 in a day\n",
      "we have 198 unique links\n"
     ]
    }
   ],
   "source": [
    "##validate that df shape[0] is equal to expected query count\n",
    "print(\"we should expect: {} articles\".format(sum(total_count)))\n",
    "print(\"we have: {} articles\".format(Bernie_df.shape[0]))\n",
    "\n",
    "\n",
    "## test 100 results theory\n",
    "over_100 = sum([x-100 for x in total_count if x>100 ])\n",
    "print(\"{} results came into query that exceeded 100 hits in a day.  Thus we lose any hit over 100 in a day\".format(over_100))\n",
    "\n",
    "\n",
    "##unique links\n",
    "unique_array = Bernie_df.url.unique()\n",
    "print(\"we have {} unique links\".format(unique_array.shape[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicateRowsDF = Bernie_df[Bernie_df.duplicated(\"url\")]\n",
    "duplicateRowsDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bernie_df.sort_values(by=['url'])\n",
    "Bernie_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hitting newspaper 3k Api with links\n",
    "+ We can deal with duplicates and streamlining the above later\n",
    "+ to demonstrate a working product I feed our url into newspaper 3k \n",
    "+ Id say it takes 5-10 seconds per article to fetch complete text\n",
    "+ I added error check to catch videos which populates the article with a statement that just reads 'no words found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_full_text=[]\n",
    "for link in Bernie_df['url']:\n",
    "    html = requests.get(link).text\n",
    "    try: text = fulltext(html)\n",
    "    except: \n",
    "        print(\"no words found\")\n",
    "        text=\"no words found\"\n",
    "        list_full_text.append(text)\n",
    "        continue\n",
    "    list_full_text.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernie_df[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add text to previous dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernie_df['full_art']=pd.Series(list_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernie_df[:20]['full_art']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "Bernie_df.to_csv('data1'+str(time.time+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "+ we need to functionize streamline and clean up query calls.\n",
    "+ sorry my python is rusty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
